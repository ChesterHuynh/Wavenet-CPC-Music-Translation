{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"UMT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uQiDwZWXj4Ua"},"source":["# Universal Music Translation\n","*Last modified: April 21, 2021*\n","\n","This notebook sets up the UMT architecture and attempts to train it on a single batch of data.\n","\n","**References** \n","1. [Script to download raw MusicNet data (Github)](https://github.com/jthickstun/pytorch_musicnet)\n","2. [UMT's MusicNet (Github)](https://github.com/facebookresearch/music-translation)\n","3. [MusicNet Documentation](https://homes.cs.washington.edu/~thickstn/musicnet.html)"]},{"cell_type":"code","metadata":{"id":"iQzj3Cf8SRpE","executionInfo":{"status":"ok","timestamp":1620184965843,"user_tz":240,"elapsed":1669,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["import torch\n","import torch.optim as optim\n","import torch.distributed as dist\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import clip_grad_value_\n","\n","torch.backends.cudnn.benchmark = True\n","torch.multiprocessing.set_start_method('spawn', force=True)\n","\n","import os\n","import sys\n","import random\n","import subprocess\n","from subprocess import call\n","import shutil\n","from shutil import copy, move\n","import errno\n","from itertools import chain\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import tqdm\n","import re\n","import csv\n","\n","from collections import deque\n","\n","import time\n","import logging\n","from datetime import timedelta\n","\n","# UMT: data.py\n","from tempfile import NamedTemporaryFile\n","import h5py\n","import librosa\n","import torch.utils.data as data\n","from scipy.io import wavfile\n","\n","import matplotlib"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R9F_OgV1nVub"},"source":["### Downloading and Preprocessing MusicNet Data\n","1. Download and unzip raw MusicNet files\n","2. Parse files by domain and composer\n","3. Split files into train, test, val folders\n","4. Add pitch augmentation "]},{"cell_type":"code","metadata":{"id":"dUsD3HTpqT6b","executionInfo":{"status":"ok","timestamp":1620184967049,"user_tz":240,"elapsed":562,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def _check_exists(root):\n","    return os.path.exists(os.path.join(root, \"train_data\")) and \\\n","        os.path.exists(os.path.join(root, \"test_data\")) and \\\n","        os.path.exists(os.path.join(root, \"train_labels\")) and \\\n","        os.path.exists(os.path.join(root, \"test_labels\"))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CU3FP00PRufR","executionInfo":{"status":"ok","timestamp":1620184967263,"user_tz":240,"elapsed":543,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def download_data(root):\n","    \"\"\"Download MusicNet data at root.\n","    Adapted from https://github.com/jthickstun/pytorch_musicnet\n","\n","    Parameters\n","    ----------\n","    root : str, Path\n","        Directory to download MusicNet data. Will create train_data, train_labels,\n","        test_data, test_labels, and raw subdirectories.\n","    \"\"\"\n","    from six.moves import urllib\n","\n","    if not _check_exists(root):\n","        try:\n","            os.makedirs(os.path.join(root, \"raw\"))\n","        except OSError as e:\n","            if e.errno == errno.EEXIST:\n","                pass\n","            else:\n","                raise\n","        \n","        # Download musicnet.tar.gz\n","        url = \"https://homes.cs.washington.edu/~thickstn/media/musicnet.tar.gz\"\n","        filename = url.rpartition('/')[2]\n","        file_path = os.path.join(root, \"raw\", filename)\n","        if not os.path.exists(file_path):\n","            print(f\"Downloading {url}\")\n","            data = urllib.request.urlopen(url)\n","            with open(file_path, 'wb') as f:\n","                # stream the download to disk (it might not fit in memory!)\n","                while True:\n","                    chunk = data.read(16*1024)\n","                    if not chunk:\n","                        break\n","                    f.write(chunk)\n","\n","        # Unpack musicnet.tar.gz\n","        extracted_folders = [\"train_data\", \"train_labels\", \"test_data\", \"test_labels\"]\n","        if not all(map(lambda f: os.path.exists(os.path.join(root, f)), extracted_folders)):\n","            print('Extracting ' + filename)\n","            if call([\"tar\", \"-xf\", file_path, '-C', root, '--strip', '1']) != 0:\n","                raise OSError(\"Failed tarball extraction\")\n","\n","    # Download musicnet_metadata.csv\n","    url = \"https://homes.cs.washington.edu/~thickstn/media/musicnet_metadata.csv\"\n","    metadata = urllib.request.urlopen(url)\n","    with open(os.path.join(root, 'musicnet_metadata.csv'), 'wb') as f:\n","        while True:\n","            chunk = metadata.read(16*1024)\n","            if not chunk:\n","                break\n","            f.write(chunk)\n","\n","    print('Download Complete')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWh3OLV_scd_","executionInfo":{"status":"ok","timestamp":1620183191009,"user_tz":240,"elapsed":1350,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}},"outputId":"af2f35da-af54-4818-ff53-7a611008e074"},"source":["download = True\n","if download:\n","    root = '/content/musicnet'\n","    download_data(root)\n","else:\n","    print(\"Using data from Google Drive\")"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Download Complete\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7l5SIzz-pL9S","executionInfo":{"status":"ok","timestamp":1620184971131,"user_tz":240,"elapsed":846,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def parse_data(src, dst, domains):\n","    \"\"\"\n","    Extract the desired domains from the raw MusicNet files\n","\n","    Parameters\n","    ----------\n","    src: str\n","        Path to input data (e.g. /content/musicnet)\n","        \n","    \"\"\"\n","\n","    dst.mkdir(exist_ok=True, parents=True)\n","    \n","    db = pd.read_csv( src / 'musicnet_metadata.csv')\n","    traindir = src / 'train_data'\n","    testdir = src /'test_data'\n","\n","    for (ensemble, composer) in domains:\n","        fid_list = db[(db[\"composer\"] == composer) & (db[\"ensemble\"] == ensemble)].id.tolist()\n","        total_time = sum(db[(db[\"composer\"] == composer) & (db[\"ensemble\"] == ensemble)].seconds.tolist())\n","        print(f\"Total time for {composer} with {ensemble} is: {total_time} seconds\")\n","\n","\n","        domaindir = dst / f\"{composer}_{ensemble.replace(' ', '_')}\"\n","        if not os.path.exists(domaindir):\n","            os.mkdir(domaindir)\n","\n","        for fid in fid_list:\n","            fname = traindir / f'{fid}.wav'\n","            if not fname.exists():\n","                fname = testdir / f'{fid}.wav'\n","\n","            copy(str(fname), str(domaindir))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"uHkOpNwGpzlR","executionInfo":{"status":"error","timestamp":1620184971333,"user_tz":240,"elapsed":550,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}},"outputId":"f95a119c-ec47-4c27-ec3e-7154ea0739a2"},"source":["domains = [\n","        ['Accompanied Violin', 'Beethoven'],\n","        ['Solo Cello', 'Bach'],\n","        ['Solo Piano', 'Bach'],\n","        ['Solo Piano', 'Beethoven'],\n","        ['String Quartet', 'Beethoven'],\n","        ['Wind Quintet', 'Cambini'],\n","    ]\n","if download:\n","    src_path = Path('/content/musicnet')\n","else:\n","    src_path = Path('/content/gdrive/MyDrive/College/Spring 2021/DL Final Project/musicnet')\n","\n","dst_path = Path('/content/musicnet/parsed')\n","\n","parse_data(src_path, dst_path, domains)"],"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-aa4d4baba98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m'Wind Quintet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cambini'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     ]\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0msrc_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/musicnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'download' is not defined"]}]},{"cell_type":"code","metadata":{"id":"If5OdyGo_Hmp","executionInfo":{"status":"ok","timestamp":1620184974547,"user_tz":240,"elapsed":879,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["# UMT: utils.py\n","class timeit:\n","    def __init__(self, name, logger=None):\n","        self.name = name\n","        self.logger = logger\n","\n","    def __enter__(self):\n","        self.start = time.time()\n","\n","    def __exit__(self, exc_type, exc_val, exc_tb):\n","        if self.logger is None:\n","            print(f'{self.name} took {(time.time() - self.start) * 1000} ms')\n","        else:\n","            self.logger.debug('%s took %s ms', self.name, (time.time() - self.start) * 1000)\n","\n","\n","def mu_law(x, mu=255):\n","    x = np.clip(x, -1, 1)\n","    x_mu = np.sign(x) * np.log(1 + mu*np.abs(x))/np.log(1 + mu)\n","    return ((x_mu + 1)/2 * mu).astype('int16')\n","\n","\n","def inv_mu_law(x, mu=255.0):\n","    x = np.array(x).astype(np.float32)\n","    y = 2. * (x - (mu+1.)/2.) / (mu+1.)\n","    return np.sign(y) * (1./mu) * ((1. + mu)**np.abs(y) - 1.)\n","\n","\n","class LossMeter(object):\n","    def __init__(self, name):\n","        self.name = name\n","        self.losses = []\n","\n","    def reset(self):\n","        self.losses = []\n","\n","    def add(self, val):\n","        self.losses.append(val)\n","\n","    def summarize_epoch(self):\n","        if self.losses:\n","            return np.mean(self.losses)\n","        else:\n","            return 0\n","\n","    def sum(self):\n","        return sum(self.losses)\n","\n","\n","class LogFormatter:\n","    def __init__(self):\n","        self.start_time = time.time()\n","\n","    def format(self, record):\n","        elapsed_seconds = round(record.created - self.start_time)\n","\n","        prefix = \"%s - %s - %s\" % (\n","            record.levelname,\n","            time.strftime('%x %X'),\n","            timedelta(seconds=elapsed_seconds)\n","        )\n","        message = record.getMessage()\n","        message = message.replace('\\n', '\\n' + ' ' * (len(prefix) + 3))\n","        return \"%s - %s\" % (prefix, message)\n","\n","\n","def create_output_dir(opt, path: Path):\n","    if hasattr(opt, 'rank'):\n","        filepath = path / f'main_{opt.rank}.log'\n","    else:\n","        filepath = path / 'main.log'\n","\n","    print(path)\n","    if not path.exists():\n","        path.mkdir(parents=True, exist_ok=True)\n","\n","    if hasattr(opt, 'rank') and opt.rank != 0:\n","        sys.stdout = open(path / f'stdout_{opt.rank}.log', 'w')\n","        sys.stderr = open(path / f'stderr_{opt.rank}.log', 'w')\n","\n","    # Safety check\n","    if filepath.exists() and not opt.checkpoint:\n","        logging.warning(\"Experiment already exists!\")\n","\n","    # Create log formatter\n","    log_formatter = LogFormatter()\n","\n","    # Create logger and set level to debug\n","    logger = logging.getLogger()\n","    logger.handlers = []\n","    logger.setLevel(logging.DEBUG)\n","    logger.propagate = False\n","\n","    # create file handler and set level to debug\n","    file_handler = logging.FileHandler(filepath, \"a\")\n","    file_handler.setLevel(logging.DEBUG)\n","    file_handler.setFormatter(log_formatter)\n","    logger.addHandler(file_handler)\n","\n","    # create console handler and set level to info\n","    if hasattr(opt, 'rank') and opt.rank == 0:\n","        console_handler = logging.StreamHandler()\n","        console_handler.setLevel(logging.INFO)\n","        console_handler.setFormatter(log_formatter)\n","        logger.addHandler(console_handler)\n","\n","    # reset logger elapsed time\n","    def reset_time():\n","        log_formatter.start_time = time.time()\n","    logger.reset_time = reset_time\n","\n","    logger.info(opt)\n","    return logger\n","\n","\n","def setup_logger(logger_name, filename):\n","    logger = logging.getLogger(logger_name)\n","    logger.handlers = []\n","    logger.setLevel(logging.DEBUG)\n","    logger.propagate = False\n","\n","    stderr_handler = logging.StreamHandler(sys.stderr)\n","    file_handler = logging.FileHandler(filename)\n","    file_handler.setLevel(logging.DEBUG)\n","    if \"RANK\" in os.environ and os.environ[\"RANK\"] != \"0\":\n","        stderr_handler.setLevel(logging.WARNING)\n","    else:\n","        stderr_handler.setLevel(logging.INFO)\n","    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n","    stderr_handler.setFormatter(formatter)\n","    file_handler.setFormatter(formatter)\n","    logger.addHandler(stderr_handler)\n","    logger.addHandler(file_handler)\n","    return logger\n","\n","\n","def wrap(data, **kwargs):\n","    if torch.is_tensor(data):\n","        var = data.cuda(non_blocking=True)\n","        return var\n","    else:\n","        return tuple([wrap(x, **kwargs) for x in data])\n","\n","\n","def save_audio(x, path, rate):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    wavfile.write(path, rate, x)\n","\n","\n","def save_wav_image(wav, path):\n","    path.parent.mkdir(parents=True, exist_ok=True)\n","    plt.figure(figsize=(15, 5))\n","    plt.plot(wav)\n","    plt.savefig(path)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4DVrqZb_Z2X","executionInfo":{"status":"ok","timestamp":1620184976346,"user_tz":240,"elapsed":1373,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["# UMT: data.py\n","logger = setup_logger(__name__, 'data.log')\n","\n","\n","def random_of_length(seq, length):\n","    limit = seq.size(0) - length\n","    if length < 1:\n","        # logging.warning(\"%d %s\" % (length, path))\n","        return None\n","\n","    start = random.randint(0, limit)\n","    end = start + length\n","    return seq[start: end]\n","\n","\n","class EncodedFilesDataset(data.Dataset):\n","    \"\"\"\n","    Uses ffmpeg to read a random short segment from the middle of an encoded file\n","    \"\"\"\n","    FILE_TYPES = ['mp3', 'ape', 'm4a', 'flac', 'mkv', 'wav']\n","    WAV_FREQ = 16000\n","    INPUT_FREQ = 44100\n","    FFT_SZ = 2048\n","    WINLEN = FFT_SZ - 1\n","    HOP_SZ = 80\n","\n","    def __init__(self, top, seq_len=None, file_type=None, epoch_len=10000):\n","        self.path = Path(top)\n","        self.seq_len = seq_len\n","        self.file_types = [file_type] if file_type else self.FILE_TYPES\n","        self.file_paths = self.filter_paths(self.path.glob('**/*'), self.file_types)\n","        self.epoch_len = epoch_len\n","\n","    @staticmethod\n","    def filter_paths(haystack, file_types):\n","        return [f for f in haystack\n","                if (f.is_file()\n","                    and any(f.name.endswith(suffix) for suffix in file_types)\n","                    and '__MACOSX' not in f.parts)]\n","\n","    def _random_file(self):\n","        # return np.random.choice(self.file_paths, p=self.probs)\n","        return random.choice(self.file_paths)\n","\n","    @staticmethod\n","    def _file_length(file_path):\n","        output = subprocess.run(['ffprobe',\n","                                 '-show_entries', 'format=duration',\n","                                 '-v', 'quiet',\n","                                 '-print_format', 'compact=print_section=0:nokey=1:escape=csv',\n","                                 str(file_path)],\n","                                stdout=subprocess.PIPE,\n","                                stderr=subprocess.PIPE).stdout\n","        duration = float(output)\n","\n","        return duration\n","\n","    def _file_slice(self, file_path, start_time):\n","        length_sec = self.seq_len / self.WAV_FREQ\n","        length_sec += .01  # just in case\n","        with NamedTemporaryFile() as output_file:\n","            output = subprocess.run(['ffmpeg',\n","                                     '-v', 'quiet',\n","                                     '-y',  # overwrite\n","                                     '-ss', str(start_time),\n","                                     '-i', str(file_path),\n","                                     '-t', str(length_sec),\n","                                     '-f', 'wav',\n","                                     # '-af', 'dynaudnorm',\n","                                     '-ar', str(self.WAV_FREQ),  # audio rate\n","                                     '-ac', '1',  # audio channels\n","                                     output_file.name\n","                                     ],\n","                                    stdout=subprocess.PIPE,\n","                                    stderr=subprocess.PIPE).stdout\n","            rate, wav_data = wavfile.read(output_file)\n","            assert wav_data.dtype == np.int16\n","            wav = wav_data[:self.seq_len].astype('float')\n","\n","            return wav\n","\n","    def __len__(self):\n","        return self.epoch_len\n","\n","    def __getitem__(self, _):\n","        wav = self.random_file_slice()\n","        return torch.FloatTensor(wav)\n","\n","    def random_file_slice(self):\n","        wav_data = None\n","\n","        while wav_data is None or len(wav_data) != self.seq_len:\n","            try:\n","                file, file_length_sec, start_time, wav_data = self.try_random_file_slice()\n","            except Exception as e:\n","                logger.exception('Exception %s in random_file_slice.', e)\n","\n","        # logger.debug('Sample: File: %s, File length: %s, Start time: %s',\n","        #              file, file_length_sec, start_time)\n","\n","        return wav_data\n","\n","    def try_random_file_slice(self):\n","        file = self._random_file()\n","        file_length_sec = self._file_length(file)\n","        segment_length_sec = self.seq_len / self.WAV_FREQ\n","        if file_length_sec < segment_length_sec:\n","            logger.warn('File \"%s\" has length %s, segment length is %s',\n","                        file, file_length_sec, segment_length_sec)\n","\n","        start_time = random.random() * (file_length_sec - segment_length_sec * 2)  # just in case\n","        try:\n","            wav_data = self._file_slice(file, start_time)\n","        except Exception as e:\n","            logger.info(f'Exception in file slice: {e}. '\n","                        f'File: {file}, '\n","                        f'File length: {file_length_sec}, '\n","                        f'Start time: {start_time}')\n","            raise\n","\n","        if len(wav_data) != self.seq_len:\n","            logger.warn('File \"%s\" has length %s, segment length is %s, wav data length: %s',\n","                        file, file_length_sec, segment_length_sec, len(wav_data))\n","\n","        return file, file_length_sec, start_time, wav_data\n","\n","    def dump_to_folder(self, output: Path, norm_db=False):\n","        for file_path in tqdm.tqdm(self.file_paths):\n","            output_file_path = output / file_path.relative_to(self.path).with_suffix('.h5')\n","            output_file_path.parent.mkdir(parents=True, exist_ok=True)\n","            with NamedTemporaryFile(suffix='.wav') as output_wav_file, \\\n","                    NamedTemporaryFile(suffix='.wav') as norm_file_path, \\\n","                    NamedTemporaryFile(suffix='.wav') as wav_convert_file:\n","                if norm_db:\n","                    logger.debug(f'Converting {file_path} to {wav_convert_file.name}')\n","                    subprocess.run(['ffmpeg',\n","                                    '-y',\n","                                    '-i', file_path,\n","                                    wav_convert_file.name],\n","                                   stdout=subprocess.PIPE,\n","                                   stderr=subprocess.PIPE)\n","\n","                    logger.debug(f'Companding {wav_convert_file.name} to {norm_file_path.name}')\n","                    subprocess.run(['sox',\n","                                    '-G',\n","                                    wav_convert_file.name,\n","                                    norm_file_path.name,\n","                                    'compand',\n","                                    '0.3,1',\n","                                    '6:-70,-60,-20',\n","                                    '-5',\n","                                    '-90',\n","                                    '0.2'],\n","                                   stdout=subprocess.PIPE,\n","                                   stderr=subprocess.PIPE)\n","                    input_file_path = norm_file_path.name\n","                else:\n","                    input_file_path = file_path\n","\n","                logger.debug(f'Converting {input_file_path} to {output_wav_file.name}')\n","                subprocess.run(['ffmpeg',\n","                                '-v', 'quiet',\n","                                '-y',  # overwrite\n","                                '-i', input_file_path,\n","                                # '-af', 'dynaudnorm',\n","                                '-f', 'wav',\n","                                '-ar', str(self.WAV_FREQ),  # audio rate\n","                                '-ac', '1',  # audio channels,\n","                                output_wav_file.name\n","                                ],\n","                               stdout=subprocess.PIPE,\n","                               stderr=subprocess.PIPE)\n","                try:\n","                    rate, wav_data = wavfile.read(output_wav_file.name)\n","                except ValueError:\n","                    logger.info(f'Cannot read {file_path} wav conversion')\n","                    raise\n","                    # raise\n","                assert wav_data.dtype == np.int16\n","                wav = wav_data.astype('float')\n","\n","                with h5py.File(output_file_path, 'w') as output_file:\n","                    chunk_shape = (min(10000, len(wav)),)\n","                    wav_dset = output_file.create_dataset('wav', wav.shape, dtype=wav.dtype,\n","                                                          chunks=chunk_shape)\n","                    wav_dset[...] = wav\n","\n","                logger.debug(f'Saved input {file_path} to {output_file_path}. '\n","                             f'Wav length: {wav.shape}')\n","\n","\n","class H5Dataset(data.Dataset):\n","    def __init__(self, top, seq_len, dataset_name, epoch_len=10000, augmentation=None, short=False,\n","                 whole_samples=False, cache=False):\n","        self.path = Path(top)\n","        self.seq_len = seq_len\n","        self.epoch_len = epoch_len\n","        self.short = short\n","        self.whole_samples = whole_samples\n","        self.augmentation = augmentation\n","        self.dataset_name = dataset_name\n","\n","        self.file_paths = list(self.path.glob('**/*.h5'))\n","        if self.short:\n","            self.file_paths = [self.file_paths[0]]\n","\n","        self.data_cache = {}\n","        if cache:\n","            for file_path in tqdm.tqdm(self.file_paths,\n","                                       desc=f'Reading dataset {top.parent.name}/{top.name}'):\n","                dataset = self.read_h5_file(file_path)\n","                self.data_cache[file_path] = dataset[:]\n","\n","        if not self.file_paths:\n","            logger.error(f'No files found in {self.path}')\n","\n","        logger.info(f'Dataset created. {len(self.file_paths)} files, '\n","                    f'augmentation: {self.augmentation is not None}. '\n","                    f'Path: {self.path}')\n","\n","    def __getitem__(self, _):\n","        ret = None\n","        while ret is None:\n","            try:\n","                ret = self.try_random_slice()\n","                if self.augmentation:\n","                    ret = [ret, self.augmentation(ret)]\n","                else:\n","                    ret = [ret, ret]\n","\n","                if self.dataset_name == 'wav':\n","                    ret = [mu_law(x / 2 ** 15) for x in ret]\n","            except Exception as e:\n","                logger.info('Exception %s in dataset __getitem__, path %s', e, self.path)\n","                logger.debug('Exception in H5Dataset', exc_info=True)\n","\n","        return torch.tensor(ret[0]), torch.tensor(ret[1])\n","\n","    def try_random_slice(self):\n","        h5file_path = random.choice(self.file_paths)\n","        if h5file_path in self.data_cache:\n","            dataset = self.data_cache[h5file_path]\n","        else:\n","            dataset = self.read_h5_file(h5file_path)\n","        return self.read_wav_data(dataset, h5file_path)\n","\n","    def read_h5_file(self, h5file_path):\n","        try:\n","            f = h5py.File(h5file_path, 'r')\n","        except Exception as e:\n","            logger.exception('Failed opening %s', h5file_path)\n","            raise\n","\n","        try:\n","            dataset = f[self.dataset_name]\n","        except Exception:\n","            logger.exception(f'No dataset named {self.dataset_name} in {file_path}. '\n","                             f'Available datasets are: {list(f.keys())}.')\n","\n","        return dataset\n","\n","    def read_wav_data(self, dataset, path):\n","        if self.whole_samples:\n","            data = dataset[:]\n","        else:\n","            length = dataset.shape[0]\n","\n","            if length <= self.seq_len:\n","                logger.debug('Length of %s is %s', path, length)\n","\n","            start_time = random.randint(0, length - self.seq_len)\n","            data = dataset[start_time: start_time + self.seq_len]\n","            assert data.shape[0] == self.seq_len\n","\n","        return data.T\n","\n","    def __len__(self):\n","        return self.epoch_len\n","\n","\n","class WavFrequencyAugmentation:\n","    def __init__(self, wav_freq, magnitude=0.5):\n","        self.magnitude = magnitude\n","        self.wav_freq = wav_freq\n","\n","    def __call__(self, wav):\n","        length = wav.shape[0]\n","        perturb_length = random.randint(length // 4, length // 2)\n","        perturb_start = random.randint(0, length // 2)\n","        perturb_end = perturb_start + perturb_length\n","        pitch_perturb = (np.random.rand() - 0.5) * 2 * self.magnitude\n","\n","        ret = np.concatenate([wav[:perturb_start],\n","                              librosa.effects.pitch_shift(wav[perturb_start:perturb_end],\n","                                                          self.wav_freq, pitch_perturb),\n","                              wav[perturb_end:]])\n","\n","        return ret\n","\n","\n","class DatasetSet:\n","    def __init__(self, dir: Path, seq_len, args):\n","        if args.data_aug:\n","            augmentation = WavFrequencyAugmentation(EncodedFilesDataset.WAV_FREQ, args.magnitude)\n","        else:\n","            augmentation = None\n","\n","        # Original epoch_len = 10000000000\n","        self.train_dataset = H5Dataset(dir / 'train', seq_len, epoch_len=1000000,\n","                                       dataset_name=args.h5_dataset_name, augmentation=augmentation,\n","                                       short=args.short, cache=False)\n","        self.train_loader = data.DataLoader(self.train_dataset,\n","                                            batch_size=args.batch_size,\n","                                            num_workers=args.num_workers,\n","                                            pin_memory=True)\n","\n","        self.train_iter = iter(self.train_loader)\n","\n","        # Original epoch_len = 1000000000\n","        # num_workers=args.num_workers // 10 + 1\n","        self.valid_dataset = H5Dataset(dir / 'val', seq_len, epoch_len=100000,\n","                                       dataset_name=args.h5_dataset_name, augmentation=augmentation,\n","                                       short=args.short)\n","        self.valid_loader = data.DataLoader(self.valid_dataset,\n","                                            batch_size=args.batch_size,\n","                                            num_workers=args.num_workers,\n","                                            pin_memory=True)\n","\n","        self.valid_iter = iter(self.valid_loader)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2W9JcYC5pZQ","executionInfo":{"status":"ok","timestamp":1620184976347,"user_tz":240,"elapsed":611,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["# UMT: split_dir.py\n","def copy_files(files, from_path, to_path: Path):\n","    for f in files:\n","        out_file_path = to_path / f.relative_to(from_path)\n","        out_file_path.parent.mkdir(parents=True, exist_ok=True)\n","        shutil.copy(f, out_file_path)\n","\n","\n","def split(input_path: Path, output_path: Path, train_ratio, val_ratio, filetype):\n","    if filetype:\n","        filetypes = [filetype]\n","    else:\n","        filetypes = EncodedFilesDataset.FILE_TYPES\n","\n","    input_files = EncodedFilesDataset.filter_paths(input_path.glob('**/*'), filetypes)\n","    random.shuffle(input_files)\n","\n","    logger.info(f'Found {len(input_files)} files')\n","\n","    n_train = int(len(input_files) * train_ratio)\n","    n_val = int(len(input_files) * val_ratio)\n","    if n_val == 0:\n","        n_val = 1\n","    n_test = len(input_files) - n_train - n_val\n","\n","    logger.info('Split as follows: Train - %s, Validation - %s, Test - %s', n_train, n_val, n_test)\n","    assert n_test > 0\n","\n","    copy_files(input_files[:n_train], input_path, output_path / 'train')\n","    copy_files(input_files[n_train:n_train + n_val], input_path, output_path / 'val')\n","    copy_files(input_files[n_train + n_val:], input_path, output_path / 'test')\n","\n","def split_domains(root='/content/musicnet/parsed', \n","              splitdir='/content/musicnet/split', \n","              train_ratio=0.8, val_ratio=0.1):\n","\n","    for subdir in os.scandir(root):\n","        if subdir.is_dir():\n","            input = Path(subdir.path)\n","            output = Path(os.path.join(splitdir, os.path.basename(input)))\n","            random.seed(0)\n","            split(input, output, train_ratio, val_ratio, None)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"3Ip_09txAP7a","outputId":"d1ebff17-6745-4438-c26a-51c9b9581c11"},"source":["split_domains()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-04 23:31:31,591 - INFO - Found 12 files\n","2021-05-04 23:31:31,601 - INFO - Split as follows: Train - 9, Validation - 1, Test - 2\n","2021-05-04 23:31:38,831 - INFO - Found 28 files\n","2021-05-04 23:31:38,832 - INFO - Split as follows: Train - 22, Validation - 2, Test - 4\n","2021-05-04 23:31:59,663 - INFO - Found 22 files\n","2021-05-04 23:31:59,680 - INFO - Split as follows: Train - 17, Validation - 2, Test - 3\n","2021-05-04 23:32:21,487 - INFO - Found 9 files\n","2021-05-04 23:32:21,494 - INFO - Split as follows: Train - 7, Validation - 1, Test - 1\n","2021-05-04 23:32:28,708 - INFO - Found 39 files\n","2021-05-04 23:32:28,709 - INFO - Split as follows: Train - 31, Validation - 3, Test - 5\n","2021-05-04 23:32:41,826 - INFO - Found 93 files\n","2021-05-04 23:32:41,828 - INFO - Split as follows: Train - 74, Validation - 9, Test - 10\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"aQbM1Hx5J1rC","outputId":"f62a196e-eb79-4ee1-a689-f8f86df22b62"},"source":["# UMT: preprocess.py\n","dataset = EncodedFilesDataset('/content/musicnet/split')\n","preprocdir = '/content/musicnet/preprocessed'\n","dataset.dump_to_folder(preprocdir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 203/203 [03:31<00:00,  1.04s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5G4_86jUXLJS"},"source":["### WaveNet Autoencoder Architecture"]},{"cell_type":"code","metadata":{"id":"_LlnEvJoRdkq","executionInfo":{"status":"ok","timestamp":1620184979685,"user_tz":240,"elapsed":573,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class CausalConv1d(nn.Conv1d):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=2,\n","                 dilation=1,\n","                 **kwargs):\n","        super(CausalConv1d, self).__init__(\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            padding=dilation * (kernel_size - 1),\n","            dilation=dilation,\n","            **kwargs)\n","\n","    def forward(self, input):\n","        out = super(CausalConv1d, self).forward(input)\n","        return out[:, :, :-self.padding[0]]\n","\n","\n","class WavenetLayer(nn.Module):\n","    def __init__(self, residual_channels, skip_channels, cond_channels,\n","                 kernel_size=2, dilation=1):\n","        super(WavenetLayer, self).__init__()\n","\n","        self.causal = CausalConv1d(residual_channels, 2 * residual_channels,\n","                                   kernel_size, dilation=dilation, bias=True)\n","        self.condition = nn.Conv1d(cond_channels, 2 * residual_channels,\n","                                   kernel_size=1, bias=True)\n","        self.residual = nn.Conv1d(residual_channels, residual_channels,\n","                                  kernel_size=1, bias=True)\n","        self.skip = nn.Conv1d(residual_channels, skip_channels,\n","                              kernel_size=1, bias=True)\n","\n","    def _condition(self, x, c, f):\n","        c = f(c)\n","        x = x + c\n","        return x\n","\n","    def forward(self, x, c=None):\n","        x = self.causal(x)\n","        if c is not None:\n","            x = self._condition(x, c, self.condition)\n","\n","        assert x.size(1) % 2 == 0\n","        gate, output = x.chunk(2, 1)\n","        gate = torch.sigmoid(gate)\n","        output = torch.tanh(output)\n","        x = gate * output\n","\n","        residual = self.residual(x)\n","        skip = self.skip(x)\n","\n","        return residual, skip\n","\n","\n","class WaveNet(nn.Module):\n","    def __init__(self, args, create_layers=True, shift_input=True):\n","        super().__init__()\n","\n","        self.blocks = args.blocks\n","        self.layer_num = args.layers\n","        self.kernel_size = args.kernel_size\n","        self.skip_channels = args.skip_channels\n","        self.residual_channels = args.residual_channels\n","        self.cond_channels = args.latent_d\n","        self.classes = 256\n","        self.shift_input = shift_input\n","\n","        if create_layers:\n","            layers = []\n","            for _ in range(self.blocks):\n","                for i in range(self.layer_num):\n","                    dilation = 2 ** i\n","                    layers.append(WavenetLayer(self.residual_channels, self.skip_channels, self.cond_channels,\n","                                               self.kernel_size, dilation))\n","            self.layers = nn.ModuleList(layers)\n","\n","        self.first_conv = CausalConv1d(1, self.residual_channels, kernel_size=self.kernel_size)\n","        self.skip_conv = nn.Conv1d(self.residual_channels, self.skip_channels, kernel_size=1)\n","        self.condition = nn.Conv1d(self.cond_channels, self.skip_channels, kernel_size=1)\n","        self.fc = nn.Conv1d(self.skip_channels, self.skip_channels, kernel_size=1)\n","        self.logits = nn.Conv1d(self.skip_channels, self.classes, kernel_size=1)\n","\n","    def _condition(self, x, c, f):\n","        c = f(c)\n","        x = x + c\n","        return x\n","\n","    @staticmethod\n","    def _upsample_cond(x, c):\n","        bsz, channels, length = x.size()\n","        cond_bsz, cond_channels, cond_length = c.size()\n","        assert bsz == cond_bsz\n","\n","        if c.size(2) != 1:\n","            # c = c.unsqueeze(3).repeat(1, 1, 1, length // cond_length)\n","            # c = c.view(bsz, cond_channels, length)\n","            upsample = nn.Upsample(size=length)\n","            c = upsample(c)\n","\n","        return c\n","\n","    @staticmethod\n","    def shift_right(x):\n","        x = F.pad(x, (1, 0))\n","        return x[:, :, :-1]\n","\n","    def forward(self, x, c=None):\n","        if x.dim() < 3:\n","            x = x.unsqueeze(1)\n","        if (not 'Half' in x.type()) and (not 'Float' in x.type()):\n","            x = x.float()\n","\n","        x = x / 255 - 0.5\n","\n","        if self.shift_input:\n","            x = self.shift_right(x)\n","\n","        if c is not None:\n","            c = self._upsample_cond(x, c)\n","\n","        residual = self.first_conv(x)\n","        skip = self.skip_conv(residual)\n","\n","        for layer in self.layers:\n","            r, s = layer(residual, c)\n","            residual = residual + r\n","            skip = skip + s\n","\n","        skip = F.relu(skip)\n","        skip = self.fc(skip)\n","        if c is not None:\n","            skip = self._condition(skip, c, self.condition)\n","        skip = F.relu(skip)\n","        skip = self.logits(skip)\n","\n","        return skip\n","\n","    ### Weights ###\n","    def export_layer_weights(self):\n","        Wdilated, Bdilated = [], []\n","        Wres, Bres = [], []\n","        Wskip, Bskip = [], []\n","\n","        for l in self.layers:\n","            Wdilated.append(l.causal.weight)\n","            Bdilated.append(l.causal.bias)\n","\n","            Wres.append(l.residual.weight)\n","            Bres.append(l.residual.bias)\n","\n","            Wskip.append(l.skip.weight)\n","            Bskip.append(l.skip.bias)\n","\n","        return Wdilated, Bdilated, Wres, Bres, Wskip, Bskip\n","\n","    def export_embed_weights(self):\n","        inp = torch.range(0, 255) / 255 - 0.5\n","        prev = self.first_conv.weight[:, :, 0].cpu().contiguous()\n","        prev = inp.unsqueeze(1) @ prev.transpose(0, 1)\n","        prev = prev + self.first_conv.bias.cpu() / 2\n","\n","        curr = self.first_conv.weight[:, :, 1].cpu().contiguous()\n","        curr = inp.unsqueeze(1) @ curr.transpose(0, 1)\n","        curr = curr + self.first_conv.bias.cpu() / 2\n","\n","        return prev, curr\n","\n","    def export_final_weights(self):\n","        Wzi = self.skip_conv.weight\n","        Bzi = self.skip_conv.bias\n","        Wzs = self.fc.weight\n","        Bzs = self.fc.bias\n","        Wza = self.logits.weight\n","        Bza = self.logits.bias\n","\n","        return Wzi, Bzi, Wzs, Bzs, Wza, Bza"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndBrqV9sRmli","executionInfo":{"status":"ok","timestamp":1620184982731,"user_tz":240,"elapsed":365,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class QueuedConv1d(nn.Module):\n","    def __init__(self, conv, data):\n","        super().__init__()\n","        if isinstance(conv, nn.Conv1d):\n","            self.inner_conv = nn.Conv1d(conv.in_channels,\n","                                        conv.out_channels,\n","                                        conv.kernel_size)\n","            self.init_len = conv.padding[0]\n","            self.inner_conv.weight.data.copy_(conv.weight.data)\n","            self.inner_conv.bias.data.copy_(conv.bias.data)\n","\n","        elif isinstance(conv, QueuedConv1d):\n","            self.inner_conv = nn.Conv1d(conv.inner_conv.in_channels,\n","                                        conv.inner_conv.out_channels,\n","                                        conv.inner_conv.kernel_size)\n","            self.init_len = conv.init_len\n","            self.inner_conv.weight.data.copy_(conv.inner_conv.weight.data)\n","            self.inner_conv.bias.data.copy_(conv.inner_conv.bias.data)\n","\n","        self.init_queue(data)\n","\n","    def init_queue(self, data):\n","        self.queue = deque([data[:, :, 0:1]]*self.init_len,\n","                           maxlen=self.init_len)\n","\n","    def forward(self, x):\n","        y = x\n","        x = torch.cat([self.queue[0], x], dim=2)\n","        # import pdb; pdb.set_trace()\n","        self.queue.append(y)\n","\n","        return self.inner_conv(x)\n","\n","\n","class WavenetGenerator(nn.Module):\n","    Q_ZERO = 128\n","\n","    def __init__(self, wavenet: WaveNet, batch_size=1, cond_repeat=800, wav_freq=16000):\n","        super().__init__()\n","        self.wavenet = wavenet\n","        self.wavenet.shift_input = False\n","        self.cond_repeat = cond_repeat\n","        self.wav_freq = wav_freq\n","        self.batch_size = batch_size\n","        self.was_cuda = next(self.wavenet.parameters()).is_cuda\n","\n","        x = torch.zeros(self.batch_size, 1, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        self.wavenet.first_conv = QueuedConv1d(self.wavenet.first_conv, x)\n","\n","        x = torch.zeros(self.batch_size, self.wavenet.residual_channels, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        for layer in self.wavenet.layers:\n","            layer.causal = QueuedConv1d(layer.causal, x)\n","\n","        if self.was_cuda:\n","            self.wavenet.cuda()\n","        self.wavenet.eval()\n","\n","    def forward(self, x, c=None):\n","        return self.wavenet(x, c)\n","\n","    def reset(self):\n","        return self.init()\n","\n","    def init(self, batch_size=None):\n","        if batch_size is not None:\n","            self.batch_size = batch_size\n","\n","        x = torch.zeros(self.batch_size, 1, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        self.wavenet.first_conv.init_queue(x)\n","\n","        x = torch.zeros(self.batch_size, self.wavenet.residual_channels, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        for layer in self.wavenet.layers:\n","            layer.causal.init_queue(x)\n","\n","        if self.was_cuda:\n","            self.wavenet.cuda()\n","\n","    @staticmethod\n","    def softmax_and_sample(prediction, method='sample'):\n","        if method == 'sample':\n","            probabilities = F.softmax(prediction)\n","            samples = torch.multinomial(probabilities, 1)\n","        elif method == 'max':\n","            _, samples = torch.max(F.softmax(prediction), dim=1)\n","        else:\n","            assert False, \"Method not supported.\"\n","\n","        return samples\n","\n","    def generate(self, encodings, init=True, method='sample'):\n","        if init:\n","            self.init(encodings.size(0))\n","\n","        samples = torch.zeros(encodings.size(0), 1, encodings.size(2)*self.cond_repeat + 1)\n","        samples.fill_(self.Q_ZERO)\n","        samples = samples.long()\n","        samples = samples.cuda() if encodings.is_cuda else samples\n","\n","        with torch.no_grad():\n","            t0 = time.time()\n","            for t1 in tqdm.tqdm(range(encodings.size(2)), desc='Generating'):\n","                for t2 in range(self.cond_repeat):\n","                    t = t1 * self.cond_repeat + t2\n","                    x = samples[:, :, t:t + 1].clone()\n","                    c = encodings[:, :, t1:t1+1]\n","\n","                    prediction = self(x, c)[:, :, 0]\n","\n","                    argmax = self.softmax_and_sample(prediction, method)\n","\n","                    samples[:, :, t+1] = argmax\n","\n","            logging.info(f'{encodings.size(0)} samples of {encodings.size(2)*self.cond_repeat/self.wav_freq} seconds length '\n","                         f'generated in {time.time() - t0} seconds.')\n","\n","        return samples[:, :, 1:]\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"JojAmEepRupx","executionInfo":{"status":"ok","timestamp":1620184989906,"user_tz":240,"elapsed":560,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class DilatedResConv(nn.Module):\n","    def __init__(self, channels, dilation=1, activation='relu', padding=1, kernel_size=3, left_pad=0):\n","        super().__init__()\n","        in_channels = channels\n","\n","        if activation == 'relu':\n","            self.activation = lambda *args, **kwargs: F.relu(*args, **kwargs, inplace=True)\n","        elif activation == 'tanh':\n","            self.activation = F.tanh\n","        elif activation == 'glu':\n","            self.activation = F.glu\n","            in_channels = channels // 2\n","\n","        self.left_pad = left_pad\n","        self.dilated_conv = nn.Conv1d(in_channels, channels, kernel_size=kernel_size, stride=1,\n","                                      padding=dilation * padding, dilation=dilation, bias=True)\n","        self.conv_1x1 = nn.Conv1d(in_channels, channels,\n","                                  kernel_size=1, bias=True)\n","\n","    def forward(self, input):\n","        x = input\n","\n","        if self.left_pad > 0:\n","            x = F.pad(x, (self.left_pad, 0))\n","        x = self.dilated_conv(x)\n","        x = self.activation(x)\n","        x = self.conv_1x1(x)\n","\n","        return input + x\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","\n","        self.n_blocks = args.encoder_blocks\n","        self.n_layers = args.encoder_layers\n","        self.channels = args.encoder_channels\n","        self.latent_channels = args.latent_d\n","        self.activation = args.encoder_func\n","\n","        try:\n","            self.encoder_pool = args.encoder_pool\n","        except AttributeError:\n","            self.encoder_pool = 800\n","\n","        layers = []\n","        for _ in range(self.n_blocks):\n","            for i in range(self.n_layers):\n","                dilation = 2 ** i\n","                layers.append(DilatedResConv(self.channels, dilation, self.activation))\n","        self.dilated_convs = nn.Sequential(*layers)\n","\n","        self.start = nn.Conv1d(1, self.channels, kernel_size=3, stride=1,\n","                               padding=1)\n","        self.conv_1x1 = nn.Conv1d(self.channels, self.latent_channels, 1)\n","        self.pool = nn.AvgPool1d(self.encoder_pool)\n","\n","    def forward(self, x):\n","        x = x / 255 - .5\n","        if x.dim() < 3:\n","            x = x.unsqueeze(1)\n","\n","        x = self.start(x)\n","        x = self.dilated_convs(x)\n","        x = self.conv_1x1(x)\n","        x = self.pool(x)\n","\n","        return x\n","\n","\n","class ZDiscriminator(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.n_classes = args.n_datasets\n","\n","        convs = []\n","        for i in range(args.d_layers):\n","            in_channels = args.latent_d if i == 0 else args.d_channels\n","            convs.append(nn.Conv1d(in_channels, args.d_channels, 1))\n","            convs.append(nn.ELU())\n","        convs.append(nn.Conv1d(args.d_channels, self.n_classes, 1))\n","\n","        self.convs = nn.Sequential(*convs)\n","        self.dropout = nn.Dropout(p=args.p_dropout_discriminator)\n","\n","    def forward(self, z):\n","        z = self.dropout(z)\n","        logits = self.convs(z)  # (N, n_classes, L)\n","\n","        mean = logits.mean(2)\n","        return mean\n","\n","\n","def cross_entropy_loss(input, target):\n","    # input:  (batch, 256, len)\n","    # target: (batch, len)\n","\n","    batch, channel, seq = input.size()\n","\n","    input = input.transpose(1, 2).contiguous()\n","    input = input.view(-1, 256)  # (batch * seq, 256)\n","    target = target.view(-1).long()  # (batch * seq)\n","\n","    cross_entropy = F.cross_entropy(input, target, reduction='none')  # (batch * seq)\n","    return cross_entropy.reshape(batch, seq).mean(dim=1)  # (batch)\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ln2TI8_6XPg5"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"iO-ABfDgR0Q0","executionInfo":{"status":"ok","timestamp":1620184994883,"user_tz":240,"elapsed":1063,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class Trainer:\n","    def __init__(self, args):\n","        self.args = args\n","        self.args.n_datasets = len(self.args.data)\n","        self.expPath = Path('/content/checkpoints') / args.expName\n","\n","        torch.manual_seed(args.seed)\n","        torch.cuda.manual_seed(args.seed)\n","\n","        self.logger = create_output_dir(args, self.expPath)\n","        self.data = [DatasetSet(d, args.seq_len, args) for d in args.data]\n","        assert not args.distributed or len(self.data) == int(\n","            os.environ['WORLD_SIZE']), \"Number of datasets must match number of nodes\"\n","\n","        self.losses_recon = [LossMeter(f'recon {i}') for i in range(self.args.n_datasets)]\n","        self.loss_d_right = LossMeter('d')\n","        self.loss_total = LossMeter('total')\n","\n","        self.evals_recon = [LossMeter(f'recon {i}') for i in range(self.args.n_datasets)]\n","        self.eval_d_right = LossMeter('eval d')\n","        self.eval_total = LossMeter('eval total')\n","\n","        self.encoder = Encoder(args)\n","        self.decoder = WaveNet(args)\n","        self.discriminator = ZDiscriminator(args)\n","\n","        if args.checkpoint:\n","            checkpoint_args_path = os.path.dirname(args.checkpoint) + '/args.pth'\n","            checkpoint_args = torch.load(checkpoint_args_path)\n","\n","            self.start_epoch = checkpoint_args[-1] + 1\n","            states = torch.load(args.checkpoint)\n","\n","            self.encoder.load_state_dict(states['encoder_state'])\n","            self.decoder.load_state_dict(states['decoder_state'])\n","            self.discriminator.load_state_dict(states['discriminator_state'])\n","\n","            self.logger.info('Loaded checkpoint parameters')\n","        else:\n","            self.start_epoch = 0\n","\n","        if args.distributed:\n","            self.encoder.cuda()\n","            self.encoder = torch.nn.parallel.DistributedDataParallel(self.encoder)\n","            self.discriminator.cuda()\n","            self.discriminator = torch.nn.parallel.DistributedDataParallel(self.discriminator)\n","            self.logger.info('Created DistributedDataParallel')\n","        else:\n","            self.encoder = torch.nn.DataParallel(self.encoder).cuda()\n","            self.discriminator = torch.nn.DataParallel(self.discriminator).cuda()\n","        self.decoder = torch.nn.DataParallel(self.decoder).cuda()\n","\n","        self.model_optimizer = optim.Adam(chain(self.encoder.parameters(),\n","                                                self.decoder.parameters()),\n","                                          lr=args.lr)\n","        self.d_optimizer = optim.Adam(self.discriminator.parameters(),\n","                                      lr=args.lr)\n","\n","        if args.checkpoint and args.load_optimizer:\n","            self.model_optimizer.load_state_dict(states['model_optimizer_state'])\n","            self.d_optimizer.load_state_dict(states['d_optimizer_state'])\n","\n","        self.lr_manager = torch.optim.lr_scheduler.ExponentialLR(self.model_optimizer, args.lr_decay)\n","        self.lr_manager.last_epoch = self.start_epoch\n","        #self.lr_manager.step()\n","\n","    def eval_batch(self, x, x_aug, dset_num):\n","        x, x_aug = x.float(), x_aug.float()\n","\n","        z = self.encoder(x)\n","        y = self.decoder(x, z)\n","        z_logits = self.discriminator(z)\n","\n","        z_classification = torch.max(z_logits, dim=1)[1]\n","\n","        z_accuracy = (z_classification == dset_num).float().mean()\n","\n","        self.eval_d_right.add(z_accuracy.data.item())\n","\n","        # discriminator_right = F.cross_entropy(z_logits, dset_num).mean()\n","        discriminator_right = F.cross_entropy(z_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","        recon_loss = cross_entropy_loss(y, x)\n","\n","        self.evals_recon[dset_num].add(recon_loss.data.cpu().numpy().mean())\n","\n","        total_loss = discriminator_right.data.item() * self.args.d_lambda + \\\n","                     recon_loss.mean().data.item()\n","\n","        self.eval_total.add(total_loss)\n","\n","        return total_loss\n","\n","    def train_batch(self, x, x_aug, dset_num):\n","        x, x_aug = x.float(), x_aug.float()\n","\n","        # Optimize D - discriminator right\n","        z = self.encoder(x)\n","        z_logits = self.discriminator(z)\n","        discriminator_right = F.cross_entropy(z_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","        loss = discriminator_right * self.args.d_lambda\n","        self.d_optimizer.zero_grad()\n","        loss.backward()\n","        if self.args.grad_clip is not None:\n","            clip_grad_value_(self.discriminator.parameters(), self.args.grad_clip)\n","\n","        self.d_optimizer.step()\n","\n","        # optimize G - reconstructs well, discriminator wrong\n","        z = self.encoder(x_aug)\n","        y = self.decoder(x, z)\n","        z_logits = self.discriminator(z)\n","        discriminator_wrong = - F.cross_entropy(z_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","\n","        if not (-100 < discriminator_right.data.item() < 100):\n","            self.logger.debug(f'z_logits: {z_logits.detach().cpu().numpy()}')\n","            self.logger.debug(f'dset_num: {dset_num}')\n","\n","        recon_loss = cross_entropy_loss(y, x)\n","        self.losses_recon[dset_num].add(recon_loss.data.cpu().numpy().mean())\n","\n","        loss = (recon_loss.mean() + self.args.d_lambda * discriminator_wrong)\n","\n","        self.model_optimizer.zero_grad()\n","        loss.backward()\n","        if self.args.grad_clip is not None:\n","            clip_grad_value_(self.encoder.parameters(), self.args.grad_clip)\n","            clip_grad_value_(self.decoder.parameters(), self.args.grad_clip)\n","        self.model_optimizer.step()\n","\n","        self.loss_total.add(loss.data.item())\n","\n","        return loss.data.item()\n","\n","    def train_epoch(self, epoch):\n","        for meter in self.losses_recon:\n","            meter.reset()\n","        self.loss_d_right.reset()\n","        self.loss_total.reset()\n","\n","        self.encoder.train()\n","        self.decoder.train()\n","        self.discriminator.train()\n","\n","        n_batches = self.args.epoch_len\n","\n","        with tqdm.tqdm(total=n_batches, desc='Train epoch %d' % epoch) as train_enum:\n","            for batch_num in range(n_batches):\n","                if self.args.short and batch_num == 3:\n","                    break\n","\n","                if self.args.distributed:\n","                    assert self.args.rank < self.args.n_datasets, \"No. of workers must be equal to #dataset\"\n","                    # dset_num = (batch_num + self.args.rank) % self.args.n_datasets\n","                    dset_num = self.args.rank\n","                else:\n","                    dset_num = batch_num % self.args.n_datasets\n","\n","                x, x_aug = next(self.data[dset_num].train_iter)\n","\n","                x = wrap(x)\n","                x_aug = wrap(x_aug)\n","                batch_loss = self.train_batch(x, x_aug, dset_num)\n","\n","                train_enum.set_description(f'Train (loss: {batch_loss:.2f}) epoch {epoch}')\n","                train_enum.update()\n","\n","    def evaluate_epoch(self, epoch):\n","        for meter in self.evals_recon:\n","            meter.reset()\n","        self.eval_d_right.reset()\n","        self.eval_total.reset()\n","\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        self.discriminator.eval()\n","\n","        n_batches = int(np.ceil(self.args.epoch_len / 10))\n","\n","        with tqdm.tqdm(total=n_batches) as valid_enum, \\\n","                torch.no_grad():\n","            for batch_num in range(n_batches):\n","                if self.args.short and batch_num == 10:\n","                    break\n","\n","                if self.args.distributed:\n","                    assert self.args.rank < self.args.n_datasets, \"No. of workers must be equal to #dataset\"\n","                    dset_num = self.args.rank\n","                else:\n","                    dset_num = batch_num % self.args.n_datasets\n","\n","                x, x_aug = next(self.data[dset_num].valid_iter)\n","\n","                x = wrap(x)\n","                x_aug = wrap(x_aug)\n","                batch_loss = self.eval_batch(x, x_aug, dset_num)\n","\n","                valid_enum.set_description(f'Test (loss: {batch_loss:.2f}) epoch {epoch}')\n","                valid_enum.update()\n","\n","    @staticmethod\n","    def format_losses(meters):\n","        losses = [meter.summarize_epoch() for meter in meters]\n","        return ', '.join('{:.4f}'.format(x) for x in losses)\n","\n","    def train_losses(self):\n","        meters = [*self.losses_recon, self.loss_d_right]\n","        return self.format_losses(meters)\n","\n","    def eval_losses(self):\n","        meters = [*self.evals_recon, self.eval_d_right]\n","        return self.format_losses(meters)\n","\n","    def train(self):\n","        best_eval = float('inf')\n","\n","        # Begin!\n","        for epoch in range(self.start_epoch, self.start_epoch + self.args.epochs):\n","            self.logger.info(f'Starting epoch, Rank {self.args.rank}, Dataset: {self.args.data[self.args.rank]}')\n","            self.train_epoch(epoch)\n","            self.evaluate_epoch(epoch)\n","\n","            self.logger.info(f'Epoch %s Rank {self.args.rank} - Train loss: (%s), Test loss (%s)',\n","                             epoch, self.train_losses(), self.eval_losses())\n","            self.lr_manager.step()\n","            val_loss = self.eval_total.summarize_epoch()\n","\n","            # TODO: args.rank is always 0, need to change to prevent overwriting\n","            if val_loss < best_eval:\n","                self.save_model(f'bestmodel_{self.args.rank}.pth')\n","                best_eval = val_loss\n","\n","            if not self.args.per_epoch:\n","                self.save_model(f'lastmodel_{self.args.rank}.pth')\n","            else:\n","                self.save_model(f'lastmodel_{epoch}_rank_{self.args.rank}.pth')\n","\n","            if self.args.is_master:\n","                torch.save([self.args,\n","                            epoch],\n","                           '%s/args.pth' % self.expPath)\n","\n","            self.logger.debug('Ended epoch')\n","\n","    def save_model(self, filename):\n","        print(\"save model called\")\n","        print(self.expPath)\n","        print(filename)\n","        save_path = self.expPath / filename\n","\n","        # TODO: verify whether we should be using args.rank here\n","        torch.save({'encoder_state': self.encoder.module.state_dict(),\n","                    'decoder_state': self.decoder.module.state_dict(),\n","                    'discriminator_state': self.discriminator.module.state_dict(),\n","                    'model_optimizer_state': self.model_optimizer.state_dict(),\n","                    'dataset': self.args.rank,\n","                    'd_optimizer_state': self.d_optimizer.state_dict()\n","                    },\n","                   save_path)\n","\n","        self.logger.debug(f'Saved model to {save_path}')"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"kusPAw_FXTCB","executionInfo":{"status":"ok","timestamp":1620185001756,"user_tz":240,"elapsed":377,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class TrainerArgs:\n","    def __init__(self, data, epochs=10000, seed=1, expName='musicnet', checkpoint='', load_optimizer=None, per_epoch=False,\n","                 dist_url='env://', dist_backend='nccl', local_rank=0,\n","                 seq_len=16000, epoch_len=10000, batch_size=32, num_workers=10, data_aug=True, magnitude=0.5, lr=1e-4, lr_decay=0.98, short=False, h5_dataset_name='wav',\n","                 latent_d=128, repeat_num=6, encoder_channels=128, encoder_blocks=3, encoder_pool=800, encoder_final_kernel_size=1, encoder_layers=10, encoder_func='relu',\n","                 blocks=4, layers=10, kernel_size=2, residual_channels=128, skip_channels=128,\n","                 d_layers=3, d_channels=100, d_cond=1024, d_lambda=1e-2, p_dropout_discriminator=0.0, grad_clip=None, timestep=1):\n","        \n","        # Env options:\n","        self.epochs = epochs\n","        self.seed = seed\n","        self.expName = expName\n","        self.data = data\n","        self.checkpoint = checkpoint\n","        self.load_optimizer = load_optimizer\n","        self.per_epoch = per_epoch\n","\n","        # Distributed\n","        self.dist_url = dist_url\n","        self.dist_backend = dist_backend\n","        self.local_rank = local_rank\n","\n","        # Data options\n","        self.seq_len = seq_len\n","        self.epoch_len = epoch_len\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.data_aug = data_aug\n","        self.magnitude = magnitude\n","        self.lr = lr\n","        self.lr_decay = lr_decay\n","        self.short = short\n","        self.h5_dataset_name = h5_dataset_name\n","\n","        # Encoder options\n","        self.latent_d = latent_d\n","        self.repeat_num = repeat_num\n","        self.encoder_channels = encoder_channels\n","        self.encoder_blocks = encoder_blocks\n","        self.encoder_pool = encoder_pool\n","        self.encoder_final_kernel_size = encoder_final_kernel_size\n","        self.encoder_layers = encoder_layers\n","        self.encoder_func = encoder_func\n","\n","        # Decoder options\n","        self.blocks = blocks\n","        self.layers = layers\n","        self.kernel_size = kernel_size\n","        self.residual_channels = residual_channels\n","        self.skip_channels = skip_channels\n","\n","        # Z discriminator options\n","        self.d_layers = d_layers\n","        self.d_channels = d_channels\n","        self.d_cond = d_cond\n","        self.d_lambda = d_lambda\n","        self.p_dropout_discriminator = p_dropout_discriminator\n","        self.grad_clip = grad_clip\n","\n","        # CPC options\n","        self.timestep = timestep\n","        \n","        self.distributed = False\n","        if 'WORLD_SIZE' in os.environ:\n","            self.distributed = int(os.environ['WORLD_SIZE']) > 1\n","\n","        if self.distributed:\n","            if int(os.environ['RANK']) == 0:\n","                self.is_master = True\n","            else:\n","                self.is_master = False\n","            self.rank = int(os.environ['RANK'])\n","\n","            print('Before init_process_group')\n","            dist.init_process_group(backend=self.dist_backend,\n","                                    init_method=self.dist_url)\n","        else:\n","            self.rank = 0\n","            self.is_master = True"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"aS_2UjiUR_jS","executionInfo":{"status":"aborted","timestamp":1620183341125,"user_tz":240,"elapsed":96247,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["# num_workers must be set to 0, otherwise DataLoader exits with an unexpected error\n","# CUDA runs out of memory when batch_size=9\n","\n","data_paths = [Path('musicnet/preprocessed/Bach_Solo_Cello'), \n","        Path('musicnet/preprocessed/Beethoven_Solo_Piano'),\n","        Path('musicnet/preprocessed/Bach_Solo_Piano')]\n","\n","# data_paths = [Path('musicnet/preprocessed/Solo_Cello'), \n","#         Path('musicnet/preprocessed/Solo_Piano'),\n","#         Path('musicnet/preprocessed/Solo_Flute')]\n","args = TrainerArgs(data_paths, epochs=1, batch_size=8, lr_decay=0.995, epoch_len=1,\n","                  num_workers=0, lr=1e-3, seq_len=12000, d_lambda=1e-2, expName='musicnet',\n","                  latent_d=64, layers=14, blocks=4, data_aug=True, grad_clip=1)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.multiprocessing.set_start_method('spawn', force=True)\n","\n","trainer = Trainer(args)\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ELKXWuXwgG8","executionInfo":{"elapsed":963494,"status":"ok","timestamp":1619272883115,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"},"user_tz":240},"outputId":"c6b35ec0-e685-44fd-b592-5dbebd6129f6"},"source":["!ls -l checkpoints/musicnet"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total 201996\n","-rw-r--r-- 1 root root      1519 Apr 24 14:01 args.pth\n","-rw-r--r-- 1 root root 103055365 Apr 24 14:01 bestmodel_0.pth\n","-rw-r--r-- 1 root root 103055365 Apr 24 14:01 lastmodel_0.pth\n","-rw-r--r-- 1 root root    718972 Apr 24 14:01 main_0.log\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mh910YeYVBnh"},"source":["### Inference"]},{"cell_type":"code","metadata":{"id":"2SSQtrzZow33","executionInfo":{"status":"ok","timestamp":1620185011618,"user_tz":240,"elapsed":502,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class QueuedConv1d(nn.Module):\n","    def __init__(self, conv, data):\n","        super().__init__()\n","        if isinstance(conv, nn.Conv1d):\n","            self.inner_conv = nn.Conv1d(conv.in_channels,\n","                                        conv.out_channels,\n","                                        conv.kernel_size)\n","            self.init_len = conv.padding[0]\n","            self.inner_conv.weight.data.copy_(conv.weight.data)\n","            self.inner_conv.bias.data.copy_(conv.bias.data)\n","\n","        elif isinstance(conv, QueuedConv1d):\n","            self.inner_conv = nn.Conv1d(conv.inner_conv.in_channels,\n","                                        conv.inner_conv.out_channels,\n","                                        conv.inner_conv.kernel_size)\n","            self.init_len = conv.init_len\n","            self.inner_conv.weight.data.copy_(conv.inner_conv.weight.data)\n","            self.inner_conv.bias.data.copy_(conv.inner_conv.bias.data)\n","\n","        self.init_queue(data)\n","\n","    def init_queue(self, data):\n","        self.queue = deque([data[:, :, 0:1]]*self.init_len,\n","                           maxlen=self.init_len)\n","\n","    def forward(self, x):\n","        y = x\n","        x = torch.cat([self.queue[0], x], dim=2)\n","        # import pdb; pdb.set_trace()\n","        self.queue.append(y)\n","\n","        return self.inner_conv(x)\n","\n","\n","class WavenetGenerator(nn.Module):\n","    Q_ZERO = 128\n","\n","    def __init__(self, wavenet: WaveNet, batch_size=1, cond_repeat=800, wav_freq=16000):\n","        super().__init__()\n","        self.wavenet = wavenet\n","        self.wavenet.shift_input = False\n","        self.cond_repeat = cond_repeat\n","        self.wav_freq = wav_freq\n","        self.batch_size = batch_size\n","        self.was_cuda = next(self.wavenet.parameters()).is_cuda\n","\n","        x = torch.zeros(self.batch_size, 1, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        self.wavenet.first_conv = QueuedConv1d(self.wavenet.first_conv, x)\n","\n","        x = torch.zeros(self.batch_size, self.wavenet.residual_channels, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        for layer in self.wavenet.layers:\n","            layer.causal = QueuedConv1d(layer.causal, x)\n","\n","        if self.was_cuda:\n","            self.wavenet.cuda()\n","        self.wavenet.eval()\n","\n","    def forward(self, x, c=None):\n","        return self.wavenet(x, c)\n","\n","    def reset(self):\n","        return self.init()\n","\n","    def init(self, batch_size=None):\n","        if batch_size is not None:\n","            self.batch_size = batch_size\n","\n","        x = torch.zeros(self.batch_size, 1, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        self.wavenet.first_conv.init_queue(x)\n","\n","        x = torch.zeros(self.batch_size, self.wavenet.residual_channels, 1)\n","        x = x.cuda() if self.was_cuda else x\n","        for layer in self.wavenet.layers:\n","            layer.causal.init_queue(x)\n","\n","        if self.was_cuda:\n","            self.wavenet.cuda()\n","\n","    @staticmethod\n","    def softmax_and_sample(prediction, method='sample'):\n","        if method == 'sample':\n","            probabilities = F.softmax(prediction)\n","            samples = torch.multinomial(probabilities, 1)\n","        elif method == 'max':\n","            _, samples = torch.max(F.softmax(prediction), dim=1)\n","        else:\n","            assert False, \"Method not supported.\"\n","\n","        return samples\n","\n","    def generate(self, encodings, init=True, method='sample'):\n","        if init:\n","            self.init(encodings.size(0))\n","\n","        samples = torch.zeros(encodings.size(0), 1, encodings.size(2)*self.cond_repeat + 1)\n","        samples.fill_(self.Q_ZERO)\n","        samples = samples.long()\n","        samples = samples.cuda() if encodings.is_cuda else samples\n","\n","        with torch.no_grad():\n","            t0 = time.time()\n","            for t1 in tqdm.tqdm(range(encodings.size(2)), desc='Generating'):\n","                for t2 in range(self.cond_repeat):\n","                    t = t1 * self.cond_repeat + t2\n","                    x = samples[:, :, t:t + 1].clone()\n","                    c = encodings[:, :, t1:t1+1]\n","\n","                    prediction = self(x, c)[:, :, 0]\n","\n","                    argmax = self.softmax_and_sample(prediction, method)\n","\n","                    samples[:, :, t+1] = argmax\n","\n","            logging.info(f'{encodings.size(0)} samples of {encodings.size(2)*self.cond_repeat/self.wav_freq} seconds length '\n","                         f'generated in {time.time() - t0} seconds.')\n","\n","        return samples[:, :, 1:]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2VSEmKyxAp6","executionInfo":{"status":"ok","timestamp":1620185013043,"user_tz":240,"elapsed":700,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def extract_id(path):\n","    decoder_id = str(path)[:-4].split('_')[-1]\n","    return int(decoder_id)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwkCGdZixMf8","executionInfo":{"status":"ok","timestamp":1620185013260,"user_tz":240,"elapsed":673,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def data_samples(data_path, data_from_args, output, n, seq_len=80000):\n","    if data_path:\n","        dataset_paths = data_path\n","    elif data_from_args:\n","        input_args, _ = torch.load(data_from_args)\n","        dataset_paths = input_args.data\n","    else:\n","        print('Please supply either --data or --data-from-args')\n","        return\n","\n","    if dataset_paths[0].is_file():\n","        datasets = [H5Dataset(dataset_paths[0], seq_len, 'wav')]\n","    else:\n","        datasets = [H5Dataset(p / 'test', seq_len, 'wav')\n","                    for p in dataset_paths]\n","\n","    for dataset_id, dataset in enumerate(datasets):\n","        for i in tqdm.trange(n):\n","            wav_data, _ = dataset[0]\n","            wav_data = inv_mu_law(wav_data.numpy())\n","            save_audio(wav_data, output / f'{dataset_id}/{i}.wav', rate=EncodedFilesDataset.WAV_FREQ)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"FyeJNKm8VLFf","executionInfo":{"status":"ok","timestamp":1620185017485,"user_tz":240,"elapsed":413,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def run_on_files(files, output, checkpoint, decoders=[], rate=16000, batch_size=6, sample_len=None, split_size=20, output_next_to_orig=False,\n","                 skip_filter=False, py=True):\n","\n","    print('Starting')\n","    matplotlib.use('agg')\n","\n","    checkpoints = checkpoint.parent.glob(checkpoint.name + '_*.pth')\n","    checkpoints = [c for c in checkpoints if extract_id(c) in decoders]\n","    assert len(checkpoints) >= 1, \"No checkpoints found.\"\n","\n","    model_args = torch.load(checkpoint.parent / 'args.pth')[0]\n","    encoder = Encoder(model_args)\n","    encoder.load_state_dict(torch.load(checkpoints[0])['encoder_state'])\n","    encoder.eval()\n","    encoder = encoder.cuda()\n","\n","    decoders = []\n","    decoder_ids = []\n","    for checkpoint in checkpoints:\n","        decoder = WaveNet(model_args)\n","        decoder.load_state_dict(torch.load(checkpoint)['decoder_state'])\n","        decoder.eval()\n","        decoder = decoder.cuda()\n","        if py:\n","            decoder = WavenetGenerator(decoder, batch_size, wav_freq=rate)\n","        else:\n","            decoder = NVWavenetGenerator(decoder, rate * (split_size // 20), batch_size, 3)\n","\n","        decoders += [decoder]\n","        decoder_ids += [extract_id(checkpoint)]\n","\n","    xs = []\n","    assert output_next_to_orig ^ (output is not None)\n","\n","    if len(files) == 1 and files[0].is_dir():\n","        top = files[0]\n","        file_paths = list(top.glob('**/*.wav')) + list(top.glob('**/*.h5'))\n","    else:\n","        file_paths = files\n","\n","    if not skip_filter:\n","        file_paths = [f for f in file_paths if not '_' in str(f.name)]\n","\n","    for file_path in file_paths:\n","        if file_path.suffix == '.wav':\n","            data, rate = librosa.load(file_path, sr=16000)\n","            assert rate == 16000\n","            data = mu_law(data)\n","        elif file_path.suffix == '.h5':\n","            data = mu_law(h5py.File(file_path, 'r')['wav'][:] / (2 ** 15))\n","            if data.shape[-1] % rate != 0:\n","                data = data[:-(data.shape[-1] % rate)]\n","            assert data.shape[-1] % rate == 0\n","            print(data.shape)\n","        else:\n","            raise Exception(f'Unsupported filetype {file_path}')\n","\n","        if sample_len:\n","            data = data[:sample_len]\n","        else:\n","            sample_len = len(data)\n","        xs.append(torch.tensor(data).unsqueeze(0).float().cuda())\n","\n","    xs = torch.stack(xs).contiguous()\n","    print(f'xs size: {xs.size()}')\n","\n","    def save(x, decoder_ix, filepath):\n","        wav = inv_mu_law(x.cpu().numpy())\n","        print(f'X size: {x.shape}')\n","        print(f'X min: {x.min()}, max: {x.max()}')\n","\n","        if output_next_to_orig:\n","            save_audio(wav.squeeze(), filepath.parent / f'{filepath.stem}_{decoder_ix}.wav', rate=rate)\n","        else:\n","            save_audio(wav.squeeze(), output / str(decoder_ix) / filepath.with_suffix('.wav').name, rate=rate)\n","\n","    yy = {}\n","    with torch.no_grad():\n","        zz = []\n","        for xs_batch in torch.split(xs, batch_size):\n","            zz += [encoder(xs_batch)]\n","        zz = torch.cat(zz, dim=0)\n","\n","        with timeit(\"Generation timer\"):\n","            for i, decoder_id in enumerate(decoder_ids):\n","                yy[decoder_id] = []\n","                decoder = decoders[i]\n","                for zz_batch in torch.split(zz, batch_size):\n","                    print(zz_batch.shape)\n","                    splits = torch.split(zz_batch, split_size, -1)\n","                    audio_data = []\n","                    decoder.reset()\n","                    for cond in tqdm.tqdm(splits):\n","                        audio_data += [decoder.generate(cond).cpu()]\n","                    audio_data = torch.cat(audio_data, -1)\n","                    yy[decoder_id] += [audio_data]\n","                yy[decoder_id] = torch.cat(yy[decoder_id], dim=0)\n","                del decoder\n","\n","    for decoder_ix, decoder_result in yy.items():\n","        for sample_result, filepath in zip(decoder_result, file_paths):\n","            save(sample_result, decoder_ix, filepath)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wjwYhjuyBLL"},"source":["checkpoint_args = Path('/content/checkpoints/musicnet/args.pth')\n","output = Path('/content/results')\n","checkpoint_lastmodel = Path('/content/checkpoints/musicnet/lastmodel')\n","\n","# Extract data samples to use as input for translation\n","data_samples(None, checkpoint_args, output, 4, 80000)\n","\n","files = [Path('/content/results')]\n","run_on_files(files, None, checkpoint_lastmodel, decoders=[0, 1, 2, 3, 4, 5], output_next_to_orig=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3d3ytyYPgge1"},"source":["# UMT-CPC"]},{"cell_type":"code","metadata":{"id":"IHmbpZXmgnNa","executionInfo":{"status":"ok","timestamp":1620185026560,"user_tz":240,"elapsed":371,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["from __future__ import print_function\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import math\n","\n","## PyTorch implementation of CDCK2 speaker classifier models\n","# CDCK2: base model from the paper 'Representation Learning with Contrastive Predictive Coding'\n","# SpkClassifier: a simple NN for speaker classification\n","\n","class CPC(nn.Module):\n","    def __init__(self, args):\n","\n","        timestep = args.timestep  \n","        seq_len = args.seq_len\n","        batch_size = args.batch_size\n","\n","        super().__init__()\n","\n","        # TODO: is it better to change #input channels for WaveNet Encoder to 512? \n","        # TODO: Wavenet Encoder data gets too small after pooling. How do we adjust the downsampling factor here?\n","        self.batch_size = batch_size\n","        self.seq_len = seq_len\n","        self.timestep = timestep\n","        self.encoder = nn.Sequential( # downsampling factor = 160\n","            nn.Conv1d(1, 512, kernel_size=10, stride=5, padding=3, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=8, stride=4, padding=2, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False), \n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.gru = nn.GRU(512, 64, num_layers=1, bidirectional=False, batch_first=True)\n","        self.Wk  = nn.ModuleList([nn.Linear(64, 512) for i in range(timestep)])\n","        self.softmax  = nn.Softmax()\n","        self.lsoftmax = nn.LogSoftmax()\n","\n","        def _weights_init(m):\n","            if isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            if isinstance(m, nn.Conv1d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm1d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # initialize gru\n","        for layer_p in self.gru._all_weights:\n","            for p in layer_p:\n","                if 'weight' in p:\n","                    nn.init.kaiming_normal_(self.gru.__getattr__(p), mode='fan_out', nonlinearity='relu')\n","\n","        self.apply(_weights_init)\n","\n","    def forward(self, x, hidden):\n","        batch = x.size()[0]\n","\n","        # Encoder downsamples x by 160\n","        #t_samples is a random index into the encoded sequence z\n","        t_samples = torch.randint(self.seq_len // 160 - self.timestep, size=(1,)).long() # randomly pick a time stamp\n","\n","        # input sequence is N*C*L, e.g. 8*1*20480\n","        x = x / 255 - .5\n","        if x.dim() < 3:\n","            x = x.unsqueeze(1)\n","        z = self.encoder(x)\n","        # encoded sequence is N*C*L, e.g. 8*512*128\n","        # reshape to N*L*C for GRU, e.g. 8*128*512\n","        z = z.transpose(1,2)\n","        encode_samples = torch.empty((self.timestep, batch, 512)).float() # e.g. size 12*8*512\n","        for k in np.arange(1, self.timestep+1):\n","            encode_samples[k-1] = z[:,t_samples+k,:].view(batch, 512) # z_t+k e.g. size 8*512\n","        forward_seq = z[:,:t_samples+1,:] # e.g. size 8*100*512\n","\n","        output, hidden = self.gru(forward_seq, hidden) # output size e.g. 8*100*256\n","        c_t = output[:,t_samples,:].view(batch, args.latent_d) # c_t e.g. size 8*256\n","        pred = torch.empty((self.timestep, batch, 512)).float() # e.g. size 12*8*512\n","        for i in np.arange(self.timestep):\n","            linear = self.Wk[i]\n","            pred[i] = linear(c_t) # Wk*c_t e.g. size 8*512\n","\n","        output = output.transpose(1,2)\n","\n","        return output, hidden, encode_samples, pred \n","\n","def InfoNCELoss(encode_samples, pred, timestep):\n","    batch = encode_samples.shape[1]\n","\n","    # TODO: figure out which dimension to take softmax over\n","    nce = 0 # average over timestep and batch\n","    correct = 0\n","    for i in np.arange(timestep):\n","        total = torch.mm(encode_samples[i], torch.transpose(pred[i], 0, 1)) # e.g. size 8*8\n","        correct += torch.sum(torch.eq(torch.argmax(F.softmax(total, dim=0), dim=0), torch.arange(0, batch))) # correct is a tensor\n","        nce += torch.sum(torch.diag(F.log_softmax(total, dim=0))) # nce is a tensor\n","    nce /= -1.*batch*timestep\n","    accuracy = 1.*correct.item()/(batch * timestep)\n","\n","    return nce"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUihiLxCks3T","executionInfo":{"status":"ok","timestamp":1620185029860,"user_tz":240,"elapsed":2289,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class UMTCPCTrainer:\n","    def __init__(self, args):\n","        self.args = args\n","        self.args.n_datasets = len(self.args.data)\n","        self.expPath = Path('/content/checkpoints') / args.expName\n","\n","        torch.manual_seed(args.seed)\n","        torch.cuda.manual_seed(args.seed)\n","\n","        self.logger = create_output_dir(args, self.expPath)\n","        self.data = [DatasetSet(d, args.seq_len, args) for d in args.data]\n","        assert not args.distributed or len(self.data) == int(\n","            os.environ['WORLD_SIZE']), \"Number of datasets must match number of nodes\"\n","\n","        self.losses_recon = [LossMeter(f'recon {i}') for i in range(self.args.n_datasets)]\n","        self.losses_nce = [LossMeter(f'nce {i}') for i in range(self.args.n_datasets)]\n","        self.loss_d_right = LossMeter('d')\n","        self.loss_total = LossMeter('total')\n","\n","        self.evals_recon = [LossMeter(f'recon {i}') for i in range(self.args.n_datasets)]\n","        self.evals_nce = [LossMeter(f'nce {i}') for i in range(self.args.n_datasets)]\n","        self.eval_d_right = LossMeter('eval d')\n","        self.eval_total = LossMeter('eval total')\n","\n","        self.cpc_encoder = CPC(args)\n","        self.encoder = Encoder(args)\n","        self.decoder = WaveNet(args)\n","        self.discriminator = ZDiscriminator(args)\n","\n","        if args.distributed:\n","            self.decoder = WaveNet(args)\n","        else:\n","            self.decoders = torch.nn.ModuleList([WaveNet(args) for _ in range(self.args.n_datasets)])\n","\n","        if args.checkpoint:\n","            checkpoint_args_path = os.path.dirname(args.checkpoint) + '/args.pth'\n","            checkpoint_args = torch.load(checkpoint_args_path)\n","\n","            self.start_epoch = checkpoint_args[-1] + 1\n","            if args.distributed:\n","                states = torch.load(args.checkpoint)\n","            else:\n","                states = [torch.load(args.checkpoint + f'_{i}.pth')\n","                          for i in range(self.args.n_datasets)]\n","            if args.distributed:\n","                #self.encoder.load_state_dict(states['encoder_state'])\n","                self.cpc_encoder.load_state_dict(states['cpc_encoder_state'])\n","                self.decoder.load_state_dict(states['decoder_state'])\n","                self.discriminator.load_state_dict(states['discriminator_state'])\n","            else:\n","                #self.encoder.load_state_dict(states[0]['encoder_state'])\n","                self.cpc_encoder.load_state_dict(states['cpc_encoder_state'])\n","                for i in range(self.args.n_datasets):\n","                    self.decoders[i].load_state_dict(states[i]['decoder_state'])\n","                self.discriminator.load_state_dict(states[0]['discriminator_state'])\n","\n","            self.logger.info('Loaded checkpoint parameters')\n","        else:\n","            self.start_epoch = 0\n","\n","        ## BUGFIX Data loading ##\n","        if args.distributed:\n","            self.cpc_encoder = torch.nn.parallel.DistributedDataParallel(self.cpc_encoder)\n","            self.cpc_encoder.cuda()\n","            #self.encoder.cuda()\n","            # self.encoder = torch.nn.parallel.DistributedDataParallel(self.encoder)\n","            self.discriminator.cuda()\n","            self.discriminator = torch.nn.parallel.DistributedDataParallel(self.discriminator)\n","            self.decoder = torch.nn.DataParallel(self.decoder).cuda()\n","            self.logger.info('Created DistributedDataParallel')\n","            self.model_optimizer = optim.Adam(chain(self.cpc_encoder.parameters(),\n","                                                    self.decoder.parameters()),\n","                                              lr=args.lr)\n","        else:\n","            self.cpc_encoder = torch.nn.DataParallel(self.cpc_encoder).cuda()\n","            #self.encoder = torch.nn.DataParallel(self.encoder).cuda()\n","            self.discriminator = torch.nn.DataParallel(self.discriminator).cuda()\n","            ## BUGFIX -- IMPLEMENTED Separate optim / decoder ##\n","            self.model_optimizers = []\n","            for i, decoder in enumerate(self.decoders):\n","                self.decoders[i] = torch.nn.DataParallel(decoder).cuda()\n","            self.model_optimizers = [optim.Adam(chain(self.cpc_encoder.parameters(),\n","                                                      decoder.parameters()),\n","                                                lr=args.lr)\n","                                     for decoder in self.decoders]\n","\n","        self.d_optimizer = optim.Adam(self.discriminator.parameters(),\n","                                      lr=args.lr)\n","\n","        ## BUGFIX Data loading ##\n","        if args.checkpoint and args.load_optimizer:\n","            if args.distributed:\n","                self.model_optimizer.load_state_dict(states['model_optimizer_state'])\n","                self.d_optimizer.load_state_dict(states['d_optimizer_state'])\n","            else:\n","                for i in range(self.args.n_datasets):\n","                    self.model_optimizers[i].load_state_dict(states[i]['model_optimizer_state'])\n","                self.d_optimizer.load_state_dict(states[0]['d_optimizer_state'])\n","\n","        if args.distributed:\n","            self.lr_manager = torch.optim.lr_scheduler.ExponentialLR(self.model_optimizer, args.lr_decay)\n","            self.lr_manager.last_epoch = self.start_epoch\n","            self.lr_manager.step()\n","        else:\n","            self.lr_managers = []\n","            for i in range(self.args.n_datasets):\n","                self.lr_managers.append(torch.optim.lr_scheduler.ExponentialLR(self.model_optimizers[i], args.lr_decay))\n","                self.lr_managers[i].last_epoch = self.start_epoch\n","                self.lr_managers[i].step()\n","\n","    def init_hidden(self, use_gpu=True):\n","        if use_gpu: return torch.zeros(1, self.args.batch_size, self.args.latent_d).cuda()\n","        else: return torch.zeros(1, self.args.batch_size, self.args.latent_d)\n","\n","    def eval_batch(self, x, x_aug, dset_num):\n","        x, x_aug = x.float(), x_aug.float()\n","\n","        hidden = self.init_hidden()\n","\n","        c, hidden, encode_samples, pred = self.cpc_encoder(x, hidden)\n","        ## BUGFIX decoder ##\n","        if self.args.distributed:\n","            y = self.decoder(x, c)\n","        else:\n","            y = self.decoders[dset_num](x, c)\n","\n","        c_logits = self.discriminator(c)\n","\n","        c_classification = torch.max(c_logits, dim=1)[1]\n","\n","        c_accuracy = (c_classification == dset_num).float().mean()\n","\n","        self.eval_d_right.add(c_accuracy.data.item())\n","\n","        # discriminator_right = F.cross_entropy(c_logits, dset_num).mean()\n","        discriminator_right = F.cross_entropy(c_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","        recon_loss = cross_entropy_loss(y, x)\n","        self.evals_recon[dset_num].add(recon_loss.data.cpu().numpy().mean())\n","\n","        nce_loss = InfoNCELoss(encode_samples, pred, self.args.timestep)\n","        self.evals_nce[dset_num].add(nce_loss.data.cpu().numpy())\n","\n","        total_loss = discriminator_right.data.item() * self.args.d_lambda + \\\n","                     recon_loss.mean().data.item() + nce_loss.data.item()\n","\n","        self.eval_total.add(total_loss)\n","\n","        return total_loss\n","\n","    def train_batch(self, x, x_aug, dset_num):\n","        x, x_aug = x.float(), x_aug.float()\n","\n","        hidden = self.init_hidden()\n","\n","        # Optimize D - discriminator right\n","        c, _, encoder_samples, pred = self.cpc_encoder(x, hidden)\n","        c_logits = self.discriminator(c)\n","        discriminator_right = F.cross_entropy(c_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","        self.loss_d_right.add(discriminator_right.data.cpu())\n","\n","        # Get c_t for computing InfoNCE Loss\n","        nce_loss = InfoNCELoss(encoder_samples, pred, self.args.timestep)\n","        loss = discriminator_right * self.args.d_lambda + nce_loss\n","        self.d_optimizer.zero_grad()\n","        loss.backward()\n","        if self.args.grad_clip is not None:\n","            clip_grad_value_(self.discriminator.parameters(), self.args.grad_clip)\n","\n","        self.d_optimizer.step()\n","\n","        # optimize G - reconstructs well, discriminator wrong\n","        c, _, encoder_samples, pred = self.cpc_encoder(x_aug, hidden)\n","        if self.args.distributed:\n","            y = self.decoder(x, c)\n","        else:\n","            y = self.decoders[dset_num](x, c)\n","        c_logits = self.discriminator(c)\n","        discriminator_wrong = - F.cross_entropy(c_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","\n","        if not (-100 < discriminator_right.data.item() < 100):\n","            self.logger.debug(f'c_logits: {c_logits.detach().cpu().numpy()}')\n","            self.logger.debug(f'dset_num: {dset_num}')\n","\n","        nce_loss = InfoNCELoss(encoder_samples, pred, self.args.timestep)\n","        self.losses_nce[dset_num].add(nce_loss.data.cpu().numpy())\n","\n","        recon_loss = cross_entropy_loss(y, x)\n","        self.losses_recon[dset_num].add(recon_loss.data.cpu().numpy().mean())\n","\n","        loss = (recon_loss.mean() + self.args.d_lambda * discriminator_wrong) + nce_loss\n","\n","        if self.args.distributed:\n","            self.model_optimizer.zero_grad()\n","        else:\n","            self.model_optimizers[dset_num].zero_grad()\n","        loss.backward()\n","        if self.args.grad_clip is not None:\n","            clip_grad_value_(self.encoder.parameters(), self.args.grad_clip)\n","            if self.args.distributed:\n","                clip_grad_value_(self.decoder.parameters(), self.args.grad_clip)\n","            else:\n","                for decoder in self.decoders:\n","                    clip_grad_value_(decoder.parameters(), self.args.grad_clip)\n","        ## BUGFIX model optimizer ##\n","        if self.args.distributed:\n","            self.model_optimizer.step()\n","        else:\n","            self.model_optimizers[dset_num].step()\n","\n","        self.loss_total.add(loss.data.item())\n","\n","        return loss.data.item()\n","\n","    def train_epoch(self, epoch):\n","        for meter in self.losses_recon:\n","            meter.reset()\n","        self.loss_d_right.reset()\n","        self.loss_total.reset()\n","\n","        self.cpc_encoder.train()\n","        if self.args.distributed:\n","            self.decoder.train()\n","        else:\n","            for decoder in self.decoders:\n","                decoder.train()\n","        self.discriminator.train()\n","\n","        n_batches = self.args.epoch_len\n","\n","        with tqdm.tqdm(total=n_batches, desc='Train epoch %d' % epoch) as train_enum:\n","            for batch_num in range(n_batches):\n","                if self.args.short and batch_num == 3:\n","                    break\n","\n","                if self.args.distributed:\n","                    assert self.args.rank < self.args.n_datasets, \"No. of workers must be equal to #dataset\"\n","                    # dset_num = (batch_num + self.args.rank) % self.args.n_datasets\n","                    dset_num = self.args.rank\n","                else:\n","                    dset_num = batch_num % self.args.n_datasets\n","\n","                x, x_aug = next(self.data[dset_num].train_iter)\n","\n","                x = wrap(x)\n","                x_aug = wrap(x_aug)\n","                batch_loss = self.train_batch(x, x_aug, dset_num)\n","\n","                train_enum.set_description(f'Train (loss: {batch_loss:.2f}) epoch {epoch}')\n","                train_enum.update()\n","\n","    def evaluate_epoch(self, epoch):\n","        for meter in self.evals_recon:\n","            meter.reset()\n","        self.eval_d_right.reset()\n","        self.eval_total.reset()\n","\n","        self.cpc_encoder.eval()\n","        if self.args.distributed:\n","            self.decoder.eval()\n","        else:\n","            for decoder in self.decoders:\n","                decoder.eval()\n","        self.discriminator.eval()\n","\n","        n_batches = int(np.ceil(self.args.epoch_len / 10))\n","\n","        with tqdm.tqdm(total=n_batches) as valid_enum, \\\n","                torch.no_grad():\n","            for batch_num in range(n_batches):\n","                if self.args.short and batch_num == 10:\n","                    break\n","\n","                if self.args.distributed:\n","                    assert self.args.rank < self.args.n_datasets, \"No. of workers must be equal to #dataset\"\n","                    dset_num = self.args.rank\n","                else:\n","                    dset_num = batch_num % self.args.n_datasets\n","\n","                x, x_aug = next(self.data[dset_num].valid_iter)\n","\n","                x = wrap(x)\n","                x_aug = wrap(x_aug)\n","                batch_loss = self.eval_batch(x, x_aug, dset_num)\n","\n","                valid_enum.set_description(f'Test (loss: {batch_loss:.2f}) epoch {epoch}')\n","                valid_enum.update()\n","\n","    @staticmethod\n","    def format_losses(meters):\n","        losses = [meter.summarize_epoch() for meter in meters]\n","        return ', '.join('{:.4f}'.format(x) for x in losses)\n","\n","    def train_losses(self):\n","        meters = [*self.losses_recon, *self.losses_nce, self.loss_d_right]\n","        return self.format_losses(meters)\n","\n","    def eval_losses(self):\n","        meters = [*self.evals_recon, *self.evals_nce, self.eval_d_right]\n","        return self.format_losses(meters)\n","\n","    def train(self):\n","        best_eval = float('inf')\n","\n","        # Begin!\n","        for epoch in range(self.start_epoch, self.start_epoch + self.args.epochs):\n","            self.logger.info(f'Starting epoch, Rank {self.args.rank}, Dataset: {self.args.data[self.args.rank]}')\n","            self.train_epoch(epoch)\n","            self.evaluate_epoch(epoch)\n","\n","            self.logger.info(f'Epoch %s Rank {self.args.rank} - Train loss: (%s), Test loss (%s)',\n","                             epoch, self.train_losses(), self.eval_losses())\n","            if self.args.distributed:\n","                self.lr_manager.step()\n","            else:\n","                for i in range(self.args.n_datasets):\n","                    self.lr_managers[i].step()\n","            val_loss = self.eval_total.summarize_epoch()\n","\n","            if val_loss < best_eval:\n","                self.save_model(f'bestmodel_{self.args.rank}.pth')\n","                best_eval = val_loss\n","\n","            if not self.args.per_epoch:\n","                self.save_model(f'lastmodel_{self.args.rank}.pth')\n","            else:\n","                self.save_model(f'lastmodel_{epoch}_rank_{self.args.rank}.pth')\n","\n","            if self.args.is_master:\n","                torch.save([self.args,\n","                            epoch],\n","                           '%s/args.pth' % self.expPath)\n","\n","            self.logger.debug('Ended epoch')\n","\n","    def save_model(self, filename):\n","        if self.args.distributed:\n","            save_path = self.expPath / filename\n","            torch.save({'encoder_state': self.cpc_encoder.module.state_dict(),\n","                        'decoder_state': self.decoder.module.state_dict(),\n","                        'discriminator_state': self.discriminator.module.state_dict(),\n","                        'model_optimizer_state': self.model_optimizer.state_dict(),\n","                        'dataset': self.args.rank,\n","                        'd_optimizer_state': self.d_optimizer.state_dict()\n","                        },\n","                    save_path)\n","            self.logger.debug(f'Saved model to {save_path}')\n","        else:\n","            filename = re.sub('_\\d.pth$', '', filename)\n","            for i in range(self.args.n_datasets):\n","                save_path = self.expPath / f'{filename}_{i}.pth'\n","                torch.save({'encoder_state': self.cpc_encoder.module.state_dict(),\n","                            'decoder_state': self.decoders[i].module.state_dict(),\n","                            'discriminator_state': self.discriminator.module.state_dict(),\n","                            'model_optimizer_state': self.model_optimizers[i].state_dict(),\n","                            'dataset': self.args.rank,\n","                            'd_optimizer_state': self.d_optimizer.state_dict()\n","                            },\n","                        save_path)\n","                self.logger.debug(f'Saved model to {save_path}')"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ctzg_hJGiGHp"},"source":["data_paths = [Path('musicnet/preprocessed/Bach_Solo_Cello'),\n","        Path('musicnet/preprocessed/Bach_Solo_Piano')]\n","args = TrainerArgs(data_paths, epochs=5, batch_size=8, lr_decay=0.995, epoch_len=20,\n","                  num_workers=0, lr=1e-3, seq_len=12000, d_lambda=1e-2, expName='musicnet_umtcpc',\n","                  latent_d=64, layers=14, blocks=4, data_aug=True, grad_clip=1, encoder_pool=100)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CEvvrG-5mywy","executionInfo":{"elapsed":166808,"status":"ok","timestamp":1620073999337,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"},"user_tz":240},"outputId":"49b06ffc-5777-4957-8423-8b14466edba8"},"source":["model = UMTCPCTrainer(args)\n","model.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING - 05/03/21 20:30:32 - 0:05:45 - Experiment already exists!\n","INFO - 05/03/21 20:30:32 - 0:00:00 - <__main__.TrainerArgs object at 0x7fbf3292f7d0>\n","2021-05-03 20:30:32,792 - INFO - Dataset created. 9 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Cello/train\n","2021-05-03 20:30:32,795 - INFO - Dataset created. 1 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Cello/val\n","2021-05-03 20:30:32,801 - INFO - Dataset created. 31 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Piano/train\n","2021-05-03 20:30:32,804 - INFO - Dataset created. 3 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Piano/val\n"],"name":"stderr"},{"output_type":"stream","text":["/content/checkpoints/musicnet_umtcpc\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","INFO - 05/03/21 20:30:33 - 0:00:00 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 9.30) epoch 0: 100%|██████████| 20/20 [00:29<00:00,  1.47s/it]\n","Test (loss: 7.51) epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]\n","INFO - 05/03/21 20:31:03 - 0:00:31 - Epoch 0 Rank 0 - Train loss: (5.4124, 5.3460, 4.7616, 6.4367, 0.6993), Test loss (5.0875, 5.1031, 2.4324, 2.3999, 0.5000)\n","INFO - 05/03/21 20:31:05 - 0:00:33 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 6.55) epoch 1: 100%|██████████| 20/20 [00:29<00:00,  1.50s/it]\n","Test (loss: 6.72) epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\n","INFO - 05/03/21 20:31:37 - 0:01:04 - Epoch 1 Rank 0 - Train loss: (4.8585, 4.7416, 3.7682, 4.6251, 0.6964), Test loss (4.5277, 4.5389, 2.4487, 2.2881, 0.6875)\n","INFO - 05/03/21 20:31:39 - 0:01:06 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 4.93) epoch 2: 100%|██████████| 20/20 [00:31<00:00,  1.58s/it]\n","Test (loss: 5.76) epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]\n","INFO - 05/03/21 20:32:11 - 0:01:39 - Epoch 2 Rank 0 - Train loss: (4.4598, 4.4395, 3.0742, 3.7857, 0.6930), Test loss (4.1928, 4.3826, 1.8563, 1.9830, 0.6875)\n","INFO - 05/03/21 20:32:13 - 0:01:41 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 5.09) epoch 3: 100%|██████████| 20/20 [00:30<00:00,  1.50s/it]\n","Test (loss: 5.15) epoch 3: 100%|██████████| 2/2 [00:01<00:00,  1.69it/s]\n","INFO - 05/03/21 20:32:45 - 0:02:12 - Epoch 3 Rank 0 - Train loss: (4.1356, 4.2128, 2.6636, 3.1932, 0.6928), Test loss (3.9655, 4.0987, 2.5985, 1.7494, 0.5625)\n","INFO - 05/03/21 20:32:46 - 0:02:13 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 5.34) epoch 4: 100%|██████████| 20/20 [00:30<00:00,  1.50s/it]\n","Test (loss: 4.05) epoch 4: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n","INFO - 05/03/21 20:33:17 - 0:02:44 - Epoch 4 Rank 0 - Train loss: (4.0013, 3.9450, 2.3638, 2.8914, 0.6826), Test loss (3.6825, 3.8825, 2.6417, 1.4325, 0.6875)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eOx7NEezz00B"},"source":["def run_on_files_umtcpc(files, output, checkpoint, decoders=[], rate=16000, batch_size=6, sample_len=None, split_size=20, output_next_to_orig=False,\n","                 skip_filter=False, py=True):\n","\n","    print('Starting')\n","    matplotlib.use('agg')\n","\n","    checkpoints = checkpoint.parent.glob(checkpoint.name + '_*.pth')\n","    checkpoints = [c for c in checkpoints if extract_id(c) in decoders]\n","    assert len(checkpoints) >= 1, \"No checkpoints found.\"\n","\n","    model_args = torch.load(checkpoint.parent / 'args.pth')[0]\n","    cpc_encoder = CPC(model_args)\n","    cpc_encoder.load_state_dict(torch.load(checkpoints[0])['encoder_state'])\n","    cpc_encoder.eval()\n","    cpc_encoder = cpc_encoder.cuda()\n","\n","    decoders = []\n","    decoder_ids = []\n","    for checkpoint in checkpoints:\n","        decoder = WaveNet(model_args)\n","        decoder.load_state_dict(torch.load(checkpoint)['decoder_state'])\n","        decoder.eval()\n","        decoder = decoder.cuda()\n","        if py:\n","            decoder = WavenetGenerator(decoder, batch_size, wav_freq=rate)\n","        else:\n","            decoder = NVWavenetGenerator(decoder, rate * (split_size // 20), batch_size, 3)\n","\n","        decoders += [decoder]\n","        decoder_ids += [extract_id(checkpoint)]\n","\n","    xs = []\n","    assert output_next_to_orig ^ (output is not None)\n","\n","    if len(files) == 1 and files[0].is_dir():\n","        top = files[0]\n","        file_paths = list(top.glob('**/*.wav')) + list(top.glob('**/*.h5'))\n","    else:\n","        file_paths = files\n","\n","    if not skip_filter:\n","        file_paths = [f for f in file_paths if not '_' in str(f.name)]\n","\n","    for file_path in file_paths:\n","        if file_path.suffix == '.wav':\n","            data, rate = librosa.load(file_path, sr=16000)\n","            assert rate == 16000\n","            data = mu_law(data)\n","        elif file_path.suffix == '.h5':\n","            data = mu_law(h5py.File(file_path, 'r')['wav'][:] / (2 ** 15))\n","            if data.shape[-1] % rate != 0:\n","                data = data[:-(data.shape[-1] % rate)]\n","            assert data.shape[-1] % rate == 0\n","            print(data.shape)\n","        else:\n","            raise Exception(f'Unsupported filetype {file_path}')\n","\n","        if sample_len:\n","            data = data[:sample_len]\n","        else:\n","            sample_len = len(data)\n","        xs.append(torch.tensor(data).unsqueeze(0).float().cuda())\n","\n","    xs = torch.stack(xs).contiguous()\n","    print(f'xs size: {xs.size()}')\n","\n","    def save(x, decoder_ix, filepath):\n","        wav = inv_mu_law(x.cpu().numpy())\n","        print(f'X size: {x.shape}')\n","        print(f'X min: {x.min()}, max: {x.max()}')\n","\n","        if output_next_to_orig:\n","            save_audio(wav.squeeze(), filepath.parent / f'{filepath.stem}_{decoder_ix}.wav', rate=rate)\n","        else:\n","            save_audio(wav.squeeze(), output / str(decoder_ix) / filepath.with_suffix('.wav').name, rate=rate)\n","\n","    def init_hidden(size, use_gpu=True):\n","        if use_gpu: return torch.zeros(1, size, model_args.latent_d).cuda()\n","        else: return torch.zeros(1, size, model_args.latent_d)\n","    \n","    yy = {}\n","    with torch.no_grad():\n","        zz = []\n","        for xs_batch in torch.split(xs, batch_size):\n","            hidden = init_hidden(len(xs_batch))\n","            output, _, _, _ = cpc_encoder(xs_batch, hidden)\n","            print(output.shape)\n","            zz += [output]\n","        #zz = torch.cat(zz, dim=0)\n","\n","        xx = torch.split(xs, batch_size)\n","\n","        with timeit(\"Generation timer\"):\n","            for i, decoder_id in enumerate(decoder_ids):\n","                yy[decoder_id] = []\n","                decoder = decoders[i]\n","                #for zz_batch in torch.split(zz, batch_size):\n","                for zz_batch, xx_batch in zip(zz, xx):\n","                    print(f\"zz_batch.shape: {zz_batch.shape}\")\n","                    print(f\"xx_batch.shape: {xx_batch.shape}\")\n","                    audio_data = decoder(xx_batch, zz_batch).cpu()\n","                    print(f\"audio_data.shape: {audio_data.shape}\")\n","                    # splits = torch.split(zz_batch, split_size, -1)\n","                    # xx_splits = torch.split(xx_batch, split_size, -1)\n","                    # audio_data = []\n","                    # decoder.reset()\n","                    # for x, cond in tqdm.tqdm(zip(xx_splits, splits)):\n","                    #     #audio_data += [decoder.generate(cond).cpu()]\n","                    #     print(f\"cond.shape: {cond.shape}\")\n","                    #     print(f\"x.shape: {x.shape}\")\n","                    #     audio_data += [decoder(x, cond).cpu()]\n","                    # audio_data = torch.cat(audio_data, -1)\n","                    yy[decoder_id] += [audio_data]\n","                yy[decoder_id] = torch.cat(yy[decoder_id], dim=0)\n","                del decoder\n","\n","    # for decoder_ix, decoder_result in enumerate(yy):\n","    #     for sample_result, filepath in zip(decoder_result, file_paths):\n","    #         save(sample_result, decoder_ix, filepath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":613},"id":"JGNFUJd9INxo","executionInfo":{"elapsed":4020,"status":"error","timestamp":1620064163155,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"},"user_tz":240},"outputId":"53c377a0-c8c8-4f7f-b425-7c1c05fb5e61"},"source":["checkpoint_args = Path('/content/checkpoints/musicnet_umtcpc/args.pth')\n","output = Path('/content/results')\n","checkpoint_lastmodel = Path('/content/checkpoints/musicnet_umtcpc/lastmodel')\n","\n","# Extract data samples to use as input for translation\n","data_samples(None, checkpoint_args, output, 4, 80000)\n","\n","files = [Path('/content/results')]\n","run_on_files_umtcpc(files, None, checkpoint_lastmodel, decoders=[0, 1, 2], output_next_to_orig=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-03 17:49:19,437 - INFO - Dataset created. 2 files, augmentation: False. Path: musicnet/preprocessed/Bach_Solo_Cello/test\n","2021-05-03 17:49:19,442 - INFO - Dataset created. 5 files, augmentation: False. Path: musicnet/preprocessed/Bach_Solo_Piano/test\n","\n","\n","100%|██████████| 4/4 [00:00<00:00, 68.74it/s]\n","\n","\n","100%|██████████| 4/4 [00:00<00:00, 86.79it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Starting\n","xs size: torch.Size([8, 1, 80000])\n","torch.Size([6, 64, 50])\n","torch.Size([2, 64, 68])\n","zz_batch.shape: torch.Size([6, 64, 50])\n","xx_batch.shape: torch.Size([6, 1, 80000])\n","Generation timer took 2167.018413543701 ms\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-ed83057ee688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_on_files_umtcpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_lastmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_next_to_orig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-51-b9a9246f3681>\u001b[0m in \u001b[0;36mrun_on_files_umtcpc\u001b[0;34m(files, output, checkpoint, decoders, rate, batch_size, sample_len, split_size, output_next_to_orig, skip_filter, py)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"zz_batch.shape: {zz_batch.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"xx_batch.shape: {xx_batch.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0maudio_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzz_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"audio_data.shape: {audio_data.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0;31m# splits = torch.split(zz_batch, split_size, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-0893308d47c4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-d04b70bb0478>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-d04b70bb0478>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-d04b70bb0478>\u001b[0m in \u001b[0;36m_condition\u001b[0;34m(self, x, c, f)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_condition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 470.00 MiB (GPU 0; 15.90 GiB total capacity; 14.43 GiB already allocated; 245.75 MiB free; 14.78 GiB reserved in total by PyTorch)"]}]},{"cell_type":"markdown","metadata":{"id":"al5t3X0qixe8"},"source":["# UMT-CPC New"]},{"cell_type":"code","metadata":{"id":"oZDuGyyUtqZM","executionInfo":{"status":"ok","timestamp":1620185034494,"user_tz":240,"elapsed":384,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class CPC(nn.Module):\n","    \"\"\"\n","    Creates a contrastive predictive coding model with a strided convolutional \n","    encoder and GRU RNN autoregressor as described by [1] and implemented in [2].\n","\n","    References\n","    ----------\n","    [1] van der Oord et al., \"Representation Learning with Contrastive \n","        Predictive Coding\", arXiv, 2019.\n","        https://arxiv.org/abs/1807.03748\n","    [2] Lai, \"Contrastive-Predictive-Coding-PyTorch\", GitHub.\n","        https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch\n","    \"\"\"\n","    def __init__(self, args):\n","        super().__init__()\n","        self.encoder = nn.Sequential( # downsampling factor = 160\n","            nn.Conv1d(1, 512, kernel_size=10, stride=5, padding=3, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=8, stride=4, padding=2, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 1, kernel_size=1, stride=1, bias=False),\n","        )\n","        #self.ar = nn.GRU(512, args.latent_d, num_layers=1, bidirectional=False, batch_first=True)\n","        self.ar = Encoder(args)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Parameters\n","        ----------\n","            x : B x 1 x L torch.Tensor\n","                Input batch of audio sequence with B samples and length L.\n","\n","        Returns\n","        -------\n","            z : B x (L // 160) x 512 torch.Tensor\n","                Encoded representation of audio sequence with 512 channels.\n","            c : B x (L // 160) x 256 torch.Tensor\n","                Context-encoded representation of audio sequence with 256 channels.\n","        \"\"\"\n","        x = x / 255 - .5\n","        if x.dim() < 3:\n","            x = x.unsqueeze(1)\n","\n","        # Use encoder to get sequence of latent representations z_t\n","        z = self.encoder(x)\n","        #z = z.transpose(1,2)\n","\n","        # Use autoregressive model to compute context latent representation c_t\n","        #c, _ = self.ar(z)\n","        c = self.ar(z)\n","        #c = c.transpose(1, 2)\n","\n","        z = z.transpose(1, 2)\n","        return z, c"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgHK4bpWuC8h","executionInfo":{"status":"ok","timestamp":1620185035960,"user_tz":240,"elapsed":702,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class InfoNCELoss(nn.Module):\n","    \"\"\"\n","    Creates a criterion that computes the InfoNCELoss as described in [1].\n","    \n","    Parameters\n","    ----------\n","        prediction_step : int\n","            Number of steps to predict into the future using context vector c\n","\n","    References\n","    ----------\n","    [1] van der Oord et al., \"Representation Learning with Contrastive \n","        Predictive Coding\", arXiv, 2019.\n","        https://arxiv.org/abs/1807.03748\n","    \"\"\"\n","    def __init__(self, args):\n","        super().__init__()\n","        self.prediction_step = args.timestep\n","        self.Wk = nn.ModuleList(\n","            nn.Linear(args.latent_d, 1) for _ in range(self.prediction_step)\n","        )\n","\n","    def get_neg_z(self, z, k, t, n_replicates):\n","        \"\"\"\n","        Parameters\n","        ----------\n","            z : B x L x 512 torch.Tensor\n","                Encoded representation of audio sequence.\n","            k : int\n","                Number of time steps in the future for prediction\n","            t : B torch.Tensor\n","                Current time step for each sample in the batch\n","            n_replicates : int\n","                Number of repetitions of the negative sampling procedure\n","\n","        Returns\n","        -------\n","            neg_samples : B x L-1 x N_rep x 512 torch.Tensor\n","                Batch-wise average InfoNCE loss\n","        \"\"\"\n","        cur_device = z.get_device() if z.get_device() != -1 else \"cpu\"\n","\n","        neg_idx = torch.vstack([torch.cat([\n","            torch.arange(0, t_i + k),             # indices before t+k\n","            torch.arange(t_i + k + 1, z.size(1))  # indices after t+k\n","        ]) for t_i in t])\n","\n","        neg_samples = torch.vstack([z[i, neg_idx[i]].unsqueeze(0) for i in range(len(t))])\n","        neg_samples = torch.stack(\n","            [\n","                torch.index_select(neg_samples, 1, torch.randint(neg_samples.size(1), \n","                                                                 (neg_samples.size(1), )).to(cur_device))\n","                for i in range(n_replicates)\n","            ],\n","            2,\n","        )\n","        return neg_samples\n","        \n","    def forward(self, z, c, n_replicates):\n","        \"\"\"\n","        Parameters\n","        ----------\n","            z : B x L x 512 torch.Tensor\n","                Encoded representation of audio sequence.\n","            c : B x L x 256 torch.Tensor\n","                Context-encoded representation of audio sequence.\n","            n_replicates : int\n","                Number of times to make a set of negative samples.\n","        \n","        Returns\n","        -------\n","            loss : float Tensor\n","                Batch-wise average InfoNCE loss\n","        \"\"\"\n","        loss = 0\n","\n","        n_batches = z.size(0)\n","\n","        # Sample random t for each batch\n","        cur_device = z.get_device() if z.get_device() != -1 else \"cpu\"\n","        t = torch.randint(z.size(1) - self.prediction_step - 1, (n_batches,)).to(cur_device)\n","\n","        # Get context vector c_t\n","        c = c.transpose(1, 2)\n","        c_t = c[torch.arange(n_batches), t] # B x 256\n","\n","        self.Wk.to(cur_device)\n","        for k in range(1, self.prediction_step + 1):\n","            # Perform negative sampling\n","            neg_samples = self.get_neg_z(z, k, t, n_replicates)  # B x L-1 x N_rep x C\n","\n","            # Compute W_k * c_t\n","            linear = self.Wk[k - 1]  # 256 x C\n","            pred = linear(c_t) # B x C\n","\n","            # Get positive z_t+k sample\n","            pos_sample = z[torch.arange(n_batches), t+k]\n","\n","            # Positive sample: compute f_k(x_t+k, c_t)\n","            # Only take diagonal elements to get product between matched batches\n","            fk_pos = torch.diag(torch.matmul(pos_sample, pred.T)) # B (1-D tensor)\n","            fk_pos_rep = fk_pos.repeat(n_replicates).view(1, 1, n_replicates, fk_pos.size(0)) # 1 x 1 x N_rep x B\n","\n","            # Negative samples: compute f_k(x_j, c_t)\n","            # Only take diagonal elements to get products between matched batches\n","            fk_neg = torch.matmul(neg_samples, pred.T) # B x L-1 x N_rep x B\n","            fk_neg = torch.diagonal(fk_neg, dim1=0, dim2=-1).unsqueeze(0) # 1 x L-1 x N_rep x B\n","\n","            # Concatenate fk for positive and negative samples\n","            fk = torch.hstack([fk_pos_rep, fk_neg]) # 1 x L x N_rep x B\n","\n","            # Compute log softmax over all fk \n","            log_sm_fk = torch.nn.LogSoftmax(dim=1)(fk)  # 1 x L x N_rep x B\n","\n","            # Compute expected value of log softmaxes over replicates\n","            exp_log_sm_fk = torch.mean(log_sm_fk, dim=2)  # 1 x L x B    \n","\n","            # Update loss with log softmax element corresponding to positive sample\n","            loss -= exp_log_sm_fk[:, 0] # 1 x B\n","            \n","        # Divide by number of predicted steps\n","        loss /= self.prediction_step\n","\n","        # Average over batches\n","        loss = loss.sum() / n_batches\n","\n","        return loss"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXOMY4EbjVb-","executionInfo":{"status":"ok","timestamp":1620185038110,"user_tz":240,"elapsed":1733,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["class UMTCPCNewTrainer:\n","    def __init__(self, args):\n","        self.args = args\n","        self.args.n_datasets = len(self.args.data)\n","        self.expPath = Path('/content/checkpoints') / args.expName\n","\n","        torch.manual_seed(args.seed)\n","        torch.cuda.manual_seed(args.seed)\n","\n","        self.logger = create_output_dir(args, self.expPath)\n","        self.data = [DatasetSet(d, args.seq_len, args) for d in args.data]\n","        assert not args.distributed or len(self.data) == int(\n","            os.environ['WORLD_SIZE']), \"Number of datasets must match number of nodes\"\n","\n","        self.losses_recon = [LossMeter(f'recon {i}') for i in range(self.args.n_datasets)]\n","        self.losses_nce = [LossMeter(f'nce {i}') for i in range(self.args.n_datasets)]\n","        self.loss_d_right = LossMeter('d')\n","        self.loss_total = LossMeter('total')\n","\n","        self.evals_recon = [LossMeter(f'recon {i}') for i in range(self.args.n_datasets)]\n","        self.evals_nce = [LossMeter(f'nce {i}') for i in range(self.args.n_datasets)]\n","        self.eval_d_right = LossMeter('eval d')\n","        self.eval_total = LossMeter('eval total')\n","\n","        self.encoder = CPC(args)\n","        self.decoder = WaveNet(args)\n","        self.discriminator = ZDiscriminator(args)\n","\n","        if args.distributed:\n","            self.decoder = WaveNet(args)\n","        else:\n","            self.decoders = torch.nn.ModuleList([WaveNet(args) for _ in range(self.args.n_datasets)])\n","\n","        if args.checkpoint:\n","            checkpoint_args_path = os.path.dirname(args.checkpoint) + '/args.pth'\n","            checkpoint_args = torch.load(checkpoint_args_path)\n","\n","            self.start_epoch = checkpoint_args[-1] + 1\n","            if args.distributed:\n","                states = torch.load(args.checkpoint)\n","            else:\n","                states = [torch.load(args.checkpoint + f'_{i}.pth')\n","                          for i in range(self.args.n_datasets)]\n","            if args.distributed:\n","                self.encoder.load_state_dict(states['encoder_state'])\n","                self.decoder.load_state_dict(states['decoder_state'])\n","                self.discriminator.load_state_dict(states['discriminator_state'])\n","            else:\n","                self.encoder.load_state_dict(states['encoder_state'])\n","                for i in range(self.args.n_datasets):\n","                    self.decoders[i].load_state_dict(states[i]['decoder_state'])\n","                self.discriminator.load_state_dict(states[0]['discriminator_state'])\n","\n","            self.logger.info('Loaded checkpoint parameters')\n","        else:\n","            self.start_epoch = 0\n","\n","        ## BUGFIX Data loading ##\n","        if args.distributed:\n","            self.encoder.cuda()\n","            self.encoder = torch.nn.parallel.DistributedDataParallel(self.encoder)\n","            self.discriminator.cuda()\n","            self.discriminator = torch.nn.parallel.DistributedDataParallel(self.discriminator)\n","            self.decoder = torch.nn.DataParallel(self.decoder).cuda()\n","            self.logger.info('Created DistributedDataParallel')\n","            self.model_optimizer = optim.Adam(chain(self.encoder.parameters(),\n","                                                    self.decoder.parameters()),\n","                                              lr=args.lr)\n","        else:\n","            self.encoder = torch.nn.DataParallel(self.encoder).cuda()\n","            self.discriminator = torch.nn.DataParallel(self.discriminator).cuda()\n","            ## BUGFIX -- IMPLEMENTED Separate optim / decoder ##\n","            self.model_optimizers = []\n","            for i, decoder in enumerate(self.decoders):\n","                self.decoders[i] = torch.nn.DataParallel(decoder).cuda()\n","            self.model_optimizers = [optim.Adam(chain(self.encoder.parameters(),\n","                                                      decoder.parameters()),\n","                                                lr=args.lr)\n","                                     for decoder in self.decoders]\n","        self.d_optimizer = optim.Adam(self.discriminator.parameters(),\n","                                      lr=args.lr)\n","\n","        ## BUGFIX Data loading ##\n","        if args.checkpoint and args.load_optimizer:\n","            if args.distributed:\n","                self.model_optimizer.load_state_dict(states['model_optimizer_state'])\n","                self.d_optimizer.load_state_dict(states['d_optimizer_state'])\n","            else:\n","                for i in range(self.args.n_datasets):\n","                    self.model_optimizers[i].load_state_dict(states[i]['model_optimizer_state'])\n","                self.d_optimizer.load_state_dict(states[0]['d_optimizer_state'])\n","\n","        if args.distributed:\n","            self.lr_manager = torch.optim.lr_scheduler.ExponentialLR(self.model_optimizer, args.lr_decay)\n","            self.lr_manager.last_epoch = self.start_epoch\n","            self.lr_manager.step()\n","        else:\n","            self.lr_managers = []\n","            for i in range(self.args.n_datasets):\n","                self.lr_managers.append(torch.optim.lr_scheduler.ExponentialLR(self.model_optimizers[i], args.lr_decay))\n","                self.lr_managers[i].last_epoch = self.start_epoch\n","                self.lr_managers[i].step()\n","\n","    def eval_batch(self, x, x_aug, dset_num):\n","        x, x_aug = x.float(), x_aug.float()\n","\n","        z, c = self.encoder(x)\n","        ## BUGFIX decoder ##\n","        if self.args.distributed:\n","            y = self.decoder(x, c)\n","        else:\n","            y = self.decoders[dset_num](x, c)\n","\n","        c_logits = self.discriminator(c)\n","\n","        c_classification = torch.max(c_logits, dim=1)[1]\n","\n","        c_accuracy = (c_classification == dset_num).float().mean()\n","\n","        self.eval_d_right.add(c_accuracy.data.item())\n","\n","        # discriminator_right = F.cross_entropy(c_logits, dset_num).mean()\n","        discriminator_right = F.cross_entropy(c_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","        recon_loss = cross_entropy_loss(y, x)\n","        self.evals_recon[dset_num].add(recon_loss.data.cpu().numpy().mean())\n","\n","        nce_loss = InfoNCELoss(args)\n","        nce_loss_val = nce_loss(z, c, n_replicates=5)\n","        self.evals_nce[dset_num].add(nce_loss_val.data.cpu().numpy())\n","\n","        total_loss = discriminator_right.data.item() * self.args.d_lambda + \\\n","                     recon_loss.mean().data.item() + nce_loss_val.data.item()\n","\n","        self.eval_total.add(total_loss)\n","\n","        return total_loss\n","\n","    def train_batch(self, x, x_aug, dset_num):\n","        x, x_aug = x.float(), x_aug.float()\n","\n","        # Optimize D - discriminator right\n","        z, c = self.encoder(x)\n","        c_logits = self.discriminator(c)\n","        discriminator_right = F.cross_entropy(c_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","        self.loss_d_right.add(discriminator_right.data.cpu())\n","\n","        # Get c_t for computing InfoNCE Loss\n","        nce_loss = InfoNCELoss(args)\n","        nce_loss_val = nce_loss(z, c, n_replicates=5)\n","        loss = discriminator_right * self.args.d_lambda + nce_loss_val\n","        self.d_optimizer.zero_grad()\n","        loss.backward()\n","        if self.args.grad_clip is not None:\n","            clip_grad_value_(self.discriminator.parameters(), self.args.grad_clip)\n","\n","        self.d_optimizer.step()\n","\n","        # optimize G - reconstructs well, discriminator wrong\n","        z, c = self.encoder(x_aug)\n","        if self.args.distributed:\n","            y = self.decoder(x, c)\n","        else:\n","            y = self.decoders[dset_num](x, c)\n","        c_logits = self.discriminator(c)\n","        discriminator_wrong = - F.cross_entropy(c_logits, torch.tensor([dset_num] * x.size(0)).long().cuda()).mean()\n","\n","        if not (-100 < discriminator_right.data.item() < 100):\n","            self.logger.debug(f'c_logits: {c_logits.detach().cpu().numpy()}')\n","            self.logger.debug(f'dset_num: {dset_num}')\n","\n","        nce_loss = InfoNCELoss(args)\n","        nce_loss_val = nce_loss(z, c, n_replicates=5)\n","        self.losses_nce[dset_num].add(nce_loss_val.data.cpu().numpy())\n","\n","        recon_loss = cross_entropy_loss(y, x)\n","        self.losses_recon[dset_num].add(recon_loss.data.cpu().numpy().mean())\n","\n","        loss = (recon_loss.mean() + self.args.d_lambda * discriminator_wrong) + nce_loss_val\n","\n","        if self.args.distributed:\n","            self.model_optimizer.zero_grad()\n","        else:\n","            self.model_optimizers[dset_num].zero_grad()\n","        loss.backward()\n","        if self.args.grad_clip is not None:\n","            clip_grad_value_(self.encoder.parameters(), self.args.grad_clip)\n","            if self.args.distributed:\n","                clip_grad_value_(self.decoder.parameters(), self.args.grad_clip)\n","            else:\n","                for decoder in self.decoders:\n","                    clip_grad_value_(decoder.parameters(), self.args.grad_clip)\n","        ## BUGFIX model optimizer ##\n","        if self.args.distributed:\n","            self.model_optimizer.step()\n","        else:\n","            self.model_optimizers[dset_num].step()\n","\n","        self.loss_total.add(loss.data.item())\n","\n","        return loss.data.item()\n","\n","    def train_epoch(self, epoch):\n","        for meter in self.losses_recon:\n","            meter.reset()\n","        self.loss_d_right.reset()\n","        self.loss_total.reset()\n","\n","        self.encoder.train()\n","        if self.args.distributed:\n","            self.decoder.train()\n","        else:\n","            for decoder in self.decoders:\n","                decoder.train()\n","        self.discriminator.train()\n","\n","        n_batches = self.args.epoch_len\n","\n","        with tqdm.tqdm(total=n_batches, desc='Train epoch %d' % epoch) as train_enum:\n","            for batch_num in range(n_batches):\n","                if self.args.short and batch_num == 3:\n","                    break\n","\n","                if self.args.distributed:\n","                    assert self.args.rank < self.args.n_datasets, \"No. of workers must be equal to #dataset\"\n","                    # dset_num = (batch_num + self.args.rank) % self.args.n_datasets\n","                    dset_num = self.args.rank\n","                else:\n","                    dset_num = batch_num % self.args.n_datasets\n","\n","                x, x_aug = next(self.data[dset_num].train_iter)\n","\n","                x = wrap(x)\n","                x_aug = wrap(x_aug)\n","                batch_loss = self.train_batch(x, x_aug, dset_num)\n","\n","                train_enum.set_description(f'Train (loss: {batch_loss:.2f}) epoch {epoch}')\n","                train_enum.update()\n","\n","    def evaluate_epoch(self, epoch):\n","        for meter in self.evals_recon:\n","            meter.reset()\n","        self.eval_d_right.reset()\n","        self.eval_total.reset()\n","\n","        self.encoder.eval()\n","        if self.args.distributed:\n","            self.decoder.eval()\n","        else:\n","            for decoder in self.decoders:\n","                decoder.eval()\n","        self.discriminator.eval()\n","\n","        n_batches = int(np.ceil(self.args.epoch_len / 10))\n","\n","        with tqdm.tqdm(total=n_batches) as valid_enum, \\\n","                torch.no_grad():\n","            for batch_num in range(n_batches):\n","                if self.args.short and batch_num == 10:\n","                    break\n","\n","                if self.args.distributed:\n","                    assert self.args.rank < self.args.n_datasets, \"No. of workers must be equal to #dataset\"\n","                    dset_num = self.args.rank\n","                else:\n","                    dset_num = batch_num % self.args.n_datasets\n","\n","                x, x_aug = next(self.data[dset_num].valid_iter)\n","\n","                x = wrap(x)\n","                x_aug = wrap(x_aug)\n","                batch_loss = self.eval_batch(x, x_aug, dset_num)\n","\n","                valid_enum.set_description(f'Test (loss: {batch_loss:.2f}) epoch {epoch}')\n","                valid_enum.update()\n","\n","    @staticmethod\n","    def format_losses(meters):\n","        losses = [meter.summarize_epoch() for meter in meters]\n","        return ', '.join('{:.4f}'.format(x) for x in losses)\n","\n","    def train_losses(self):\n","        meters = [*self.losses_recon, *self.losses_nce, self.loss_d_right]\n","        return self.format_losses(meters)\n","\n","    def eval_losses(self):\n","        meters = [*self.evals_recon, *self.evals_nce, self.eval_d_right]\n","        return self.format_losses(meters)\n","\n","    def train(self):\n","        best_eval = float('inf')\n","\n","        # Begin!\n","        for epoch in range(self.start_epoch, self.start_epoch + self.args.epochs):\n","            self.logger.info(f'Starting epoch, Rank {self.args.rank}, Dataset: {self.args.data[self.args.rank]}')\n","            self.train_epoch(epoch)\n","            self.evaluate_epoch(epoch)\n","\n","            self.logger.info(f'Epoch %s Rank {self.args.rank} - Train loss: (%s), Test loss (%s)',\n","                             epoch, self.train_losses(), self.eval_losses())\n","            if self.args.distributed:\n","                self.lr_manager.step()\n","            else:\n","                for i in range(self.args.n_datasets):\n","                    self.lr_managers[i].step()\n","            val_loss = self.eval_total.summarize_epoch()\n","\n","            if val_loss < best_eval:\n","                self.save_model(f'bestmodel_{self.args.rank}.pth')\n","                best_eval = val_loss\n","\n","            if not self.args.per_epoch:\n","                self.save_model(f'lastmodel_{self.args.rank}.pth')\n","            else:\n","                self.save_model(f'lastmodel_{epoch}_rank_{self.args.rank}.pth')\n","\n","            if self.args.is_master:\n","                torch.save([self.args,\n","                            epoch],\n","                           '%s/args.pth' % self.expPath)\n","\n","            self.logger.debug('Ended epoch')\n","\n","    def save_model(self, filename):\n","        if self.args.distributed:\n","            save_path = self.expPath / filename\n","            torch.save({'encoder_state': self.encoder.module.state_dict(),\n","                        'decoder_state': self.decoder.module.state_dict(),\n","                        'discriminator_state': self.discriminator.module.state_dict(),\n","                        'model_optimizer_state': self.model_optimizer.state_dict(),\n","                        'dataset': self.args.rank,\n","                        'd_optimizer_state': self.d_optimizer.state_dict()\n","                        },\n","                    save_path)\n","            self.logger.debug(f'Saved model to {save_path}')\n","        else:\n","            filename = re.sub('_\\d.pth$', '', filename)\n","            for i in range(self.args.n_datasets):\n","                save_path = self.expPath / f'{filename}_{i}.pth'\n","                torch.save({'encoder_state': self.encoder.module.state_dict(),\n","                            'decoder_state': self.decoders[i].module.state_dict(),\n","                            'discriminator_state': self.discriminator.module.state_dict(),\n","                            'model_optimizer_state': self.model_optimizers[i].state_dict(),\n","                            'dataset': self.args.rank,\n","                            'd_optimizer_state': self.d_optimizer.state_dict()\n","                            },\n","                        save_path)\n","                self.logger.debug(f'Saved model to {save_path}')"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RD0clWkNlCtD","executionInfo":{"status":"ok","timestamp":1620185038111,"user_tz":240,"elapsed":223,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["data_paths = [\n","              Path('musicnet/preprocessed/Bach_Solo_Cello'),\n","        Path('musicnet/preprocessed/Bach_Solo_Piano')]\n","#pool = 100\n","args = TrainerArgs(data_paths, epochs=20, batch_size=8, lr_decay=0.995, epoch_len=20,\n","                  num_workers=0, lr=1e-3, seq_len=10000, d_lambda=1e-2, expName='musicnet_umtcpc',\n","                  latent_d=64, layers=14, blocks=4, data_aug=True, grad_clip=1, encoder_pool=1, timestep=5)\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZ8s8tGolNj5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620185424792,"user_tz":240,"elapsed":364546,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}},"outputId":"1308e978-b461-40a3-8941-6cb6b905b8c0"},"source":["model = UMTCPCNewTrainer(args)\n","model.train()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["WARNING:root:Experiment already exists!\n","INFO - 05/05/21 03:24:20 - 0:00:00 - <__main__.TrainerArgs object at 0x7f636a0fdd10>\n","2021-05-05 03:24:20,394 - INFO - Dataset created. 9 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Cello/train\n","2021-05-05 03:24:20,433 - INFO - Dataset created. 1 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Cello/val\n","2021-05-05 03:24:20,437 - INFO - Dataset created. 31 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Piano/train\n","2021-05-05 03:24:20,439 - INFO - Dataset created. 3 files, augmentation: True. Path: musicnet/preprocessed/Bach_Solo_Piano/val\n"],"name":"stderr"},{"output_type":"stream","text":["/content/checkpoints/musicnet_umtcpc\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","INFO - 05/05/21 03:24:24 - 0:00:04 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 9.41) epoch 0: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n","Test (loss: 9.37) epoch 0: 100%|██████████| 2/2 [00:00<00:00,  3.16it/s]\n","INFO - 05/05/21 03:24:41 - 0:00:21 - Epoch 0 Rank 0 - Train loss: (5.3855, 5.4266, 4.2339, 4.3137, 0.6909), Test loss (5.4017, 5.1848, 4.2284, 4.1787, 0.5000)\n","INFO - 05/05/21 03:24:43 - 0:00:23 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 8.97) epoch 1: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 8.84) epoch 1: 100%|██████████| 2/2 [00:00<00:00,  3.20it/s]\n","INFO - 05/05/21 03:24:59 - 0:00:39 - Epoch 1 Rank 0 - Train loss: (4.9224, 4.9729, 4.1866, 4.2243, 0.6846), Test loss (4.7551, 4.6926, 4.1859, 4.1594, 0.5000)\n","INFO - 05/05/21 03:25:02 - 0:00:42 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 8.27) epoch 2: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 8.40) epoch 2: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n","INFO - 05/05/21 03:25:18 - 0:00:58 - Epoch 2 Rank 0 - Train loss: (4.5664, 4.4582, 4.1665, 4.1946, 0.6851), Test loss (4.4162, 4.2600, 4.1687, 4.1496, 0.5000)\n","INFO - 05/05/21 03:25:20 - 0:01:00 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 8.39) epoch 3: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 8.25) epoch 3: 100%|██████████| 2/2 [00:00<00:00,  3.19it/s]\n","INFO - 05/05/21 03:25:36 - 0:01:16 - Epoch 3 Rank 0 - Train loss: (4.3374, 4.1084, 4.1568, 4.1778, 0.6915), Test loss (4.3317, 4.1165, 4.1592, 4.1441, 0.5000)\n","INFO - 05/05/21 03:25:38 - 0:01:18 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.95) epoch 4: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n","Test (loss: 7.82) epoch 4: 100%|██████████| 2/2 [00:00<00:00,  3.17it/s]\n","INFO - 05/05/21 03:25:54 - 0:01:34 - Epoch 4 Rank 0 - Train loss: (3.9756, 4.0209, 4.1504, 4.1677, 0.6892), Test loss (4.0855, 3.6871, 4.1549, 4.1412, 0.5000)\n","INFO - 05/05/21 03:25:57 - 0:01:37 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.64) epoch 5: 100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n","Test (loss: 7.70) epoch 5: 100%|██████████| 2/2 [00:00<00:00,  3.23it/s]\n","INFO - 05/05/21 03:26:13 - 0:01:53 - Epoch 5 Rank 0 - Train loss: (3.9760, 3.7506, 4.1467, 4.1606, 0.6902), Test loss (4.1078, 3.5573, 4.1523, 4.1395, 0.5000)\n","INFO - 05/05/21 03:26:15 - 0:01:55 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.80) epoch 6: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.61) epoch 6: 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n","INFO - 05/05/21 03:26:31 - 0:02:11 - Epoch 6 Rank 0 - Train loss: (3.8320, 3.5526, 4.1441, 4.1561, 0.6890), Test loss (3.9070, 3.4767, 4.1484, 4.1374, 0.5000)\n","INFO - 05/05/21 03:26:33 - 0:02:13 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 8.05) epoch 7: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.47) epoch 7: 100%|██████████| 2/2 [00:00<00:00,  3.34it/s]\n","INFO - 05/05/21 03:26:49 - 0:02:29 - Epoch 7 Rank 0 - Train loss: (3.5619, 3.5974, 4.1422, 4.1523, 0.6881), Test loss (3.8274, 3.3334, 4.1448, 4.1360, 0.5000)\n","INFO - 05/05/21 03:26:51 - 0:02:31 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.63) epoch 8: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.77) epoch 8: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n","INFO - 05/05/21 03:27:07 - 0:02:47 - Epoch 8 Rank 0 - Train loss: (3.4685, 3.6081, 4.1405, 4.1496, 0.6877), Test loss (3.3993, 3.6326, 4.1424, 4.1350, 0.5000)\n","INFO - 05/05/21 03:27:09 - 0:02:49 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.40) epoch 9: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n","Test (loss: 7.23) epoch 9: 100%|██████████| 2/2 [00:00<00:00,  3.34it/s]\n","INFO - 05/05/21 03:27:25 - 0:03:05 - Epoch 9 Rank 0 - Train loss: (3.5899, 3.4193, 4.1393, 4.1474, 0.6841), Test loss (3.6173, 3.0972, 4.1401, 4.1346, 0.5000)\n","INFO - 05/05/21 03:27:28 - 0:03:08 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.25) epoch 10: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.11) epoch 10: 100%|██████████| 2/2 [00:00<00:00,  3.18it/s]\n","INFO - 05/05/21 03:27:44 - 0:03:24 - Epoch 10 Rank 0 - Train loss: (3.3394, 3.1466, 4.1381, 4.1455, 0.6840), Test loss (3.4335, 2.9740, 4.1410, 4.1341, 0.5000)\n","INFO - 05/05/21 03:27:46 - 0:03:26 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.54) epoch 11: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.61) epoch 11: 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]\n","INFO - 05/05/21 03:28:02 - 0:03:42 - Epoch 11 Rank 0 - Train loss: (3.2777, 3.5375, 4.1373, 4.1440, 0.6802), Test loss (3.2080, 3.4834, 4.1411, 4.1332, 0.5000)\n","INFO - 05/05/21 03:28:03 - 0:03:43 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.31) epoch 12: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s]\n","Test (loss: 7.28) epoch 12: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n","INFO - 05/05/21 03:28:19 - 0:03:59 - Epoch 12 Rank 0 - Train loss: (3.0892, 3.3008, 4.1367, 4.1428, 0.6769), Test loss (3.0203, 3.1480, 4.1400, 4.1325, 0.5000)\n","INFO - 05/05/21 03:28:21 - 0:04:01 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.46) epoch 13: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.28) epoch 13: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n","INFO - 05/05/21 03:28:37 - 0:04:17 - Epoch 13 Rank 0 - Train loss: (3.4356, 3.3228, 4.1360, 4.1417, 0.6660), Test loss (3.9741, 3.1487, 4.1387, 4.1320, 0.5000)\n","INFO - 05/05/21 03:28:38 - 0:04:18 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.21) epoch 14: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 6.99) epoch 14: 100%|██████████| 2/2 [00:00<00:00,  3.34it/s]\n","INFO - 05/05/21 03:28:54 - 0:04:34 - Epoch 14 Rank 0 - Train loss: (3.2978, 3.0915, 4.1351, 4.1405, 0.6780), Test loss (3.1413, 2.8631, 4.1378, 4.1315, 0.5000)\n","INFO - 05/05/21 03:28:56 - 0:04:36 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 6.97) epoch 15: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n","Test (loss: 6.93) epoch 15: 100%|██████████| 2/2 [00:00<00:00,  3.31it/s]\n","INFO - 05/05/21 03:29:12 - 0:04:52 - Epoch 15 Rank 0 - Train loss: (3.1192, 2.9420, 4.1348, 4.1398, 0.6592), Test loss (3.0780, 2.7963, 4.1374, 4.1313, 0.5000)\n","INFO - 05/05/21 03:29:14 - 0:04:55 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 6.86) epoch 16: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\n","Test (loss: 6.87) epoch 16: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n","INFO - 05/05/21 03:29:30 - 0:05:10 - Epoch 16 Rank 0 - Train loss: (2.9837, 2.8873, 4.1344, 4.1390, 0.6042), Test loss (2.9910, 2.7429, 4.1373, 4.1308, 0.5000)\n","INFO - 05/05/21 03:29:33 - 0:05:13 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.25) epoch 17: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.04) epoch 17: 100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n","INFO - 05/05/21 03:29:49 - 0:05:29 - Epoch 17 Rank 0 - Train loss: (2.9294, 2.9685, 4.1340, 4.1383, 0.6327), Test loss (3.0915, 2.8953, 4.1367, 4.1308, 0.5000)\n","INFO - 05/05/21 03:29:50 - 0:05:30 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.12) epoch 18: 100%|██████████| 20/20 [00:15<00:00,  1.29it/s]\n","Test (loss: 7.08) epoch 18: 100%|██████████| 2/2 [00:00<00:00,  3.27it/s]\n","INFO - 05/05/21 03:30:06 - 0:05:46 - Epoch 18 Rank 0 - Train loss: (3.0309, 2.9777, 4.1337, 4.1379, 0.5857), Test loss (3.5332, 2.9318, 4.1363, 4.1306, 0.5000)\n","INFO - 05/05/21 03:30:07 - 0:05:47 - Starting epoch, Rank 0, Dataset: musicnet/preprocessed/Bach_Solo_Cello\n","Train (loss: 7.00) epoch 19: 100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n","Test (loss: 7.09) epoch 19: 100%|██████████| 2/2 [00:00<00:00,  3.22it/s]\n","INFO - 05/05/21 03:30:23 - 0:06:03 - Epoch 19 Rank 0 - Train loss: (3.3973, 2.9619, 4.1334, 4.1373, 0.5884), Test loss (3.1612, 2.9657, 4.1358, 4.1300, 0.5000)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"QkoRu6zR841Y","executionInfo":{"status":"ok","timestamp":1620187822546,"user_tz":240,"elapsed":376,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}}},"source":["def run_on_files_umtcpc(files, output, checkpoint, decoders=[], rate=16000, batch_size=6, sample_len=None, split_size=20, output_next_to_orig=False,\n","                 skip_filter=False, py=True):\n","\n","    print('Starting')\n","    matplotlib.use('agg')\n","\n","    checkpoints = checkpoint.parent.glob(checkpoint.name + '_*.pth')\n","    checkpoints = [c for c in checkpoints if extract_id(c) in decoders]\n","    assert len(checkpoints) >= 1, \"No checkpoints found.\"\n","\n","    model_args = torch.load(checkpoint.parent / 'args.pth')[0]\n","    encoder = CPC(model_args)\n","    encoder.load_state_dict(torch.load(checkpoints[0])['encoder_state'])\n","    encoder.eval()\n","    encoder = encoder.cuda()\n","\n","    decoders = []\n","    decoder_ids = []\n","    for checkpoint in checkpoints:\n","        decoder = WaveNet(model_args)\n","        decoder.load_state_dict(torch.load(checkpoint)['decoder_state'])\n","        decoder.eval()\n","        decoder = decoder.cuda()\n","        if py:\n","            decoder = WavenetGenerator(decoder, batch_size, wav_freq=rate)\n","        else:\n","            decoder = NVWavenetGenerator(decoder, rate * (split_size // 20), batch_size, 3)\n","\n","        decoders += [decoder]\n","        decoder_ids += [extract_id(checkpoint)]\n","\n","    xs = []\n","    assert output_next_to_orig ^ (output is not None)\n","\n","    if len(files) == 1 and files[0].is_dir():\n","        top = files[0]\n","        file_paths = list(top.glob('**/*.wav')) + list(top.glob('**/*.h5'))\n","    else:\n","        file_paths = files\n","\n","    if not skip_filter:\n","        file_paths = [f for f in file_paths if not '_' in str(f.name)]\n","\n","    for file_path in file_paths:\n","        if file_path.suffix == '.wav':\n","            data, rate = librosa.load(file_path, sr=16000)\n","            assert rate == 16000\n","            data = mu_law(data)\n","        elif file_path.suffix == '.h5':\n","            data = mu_law(h5py.File(file_path, 'r')['wav'][:] / (2 ** 15))\n","            if data.shape[-1] % rate != 0:\n","                data = data[:-(data.shape[-1] % rate)]\n","            assert data.shape[-1] % rate == 0\n","            print(data.shape)\n","        else:\n","            raise Exception(f'Unsupported filetype {file_path}')\n","\n","        if sample_len:\n","            data = data[:sample_len]\n","        else:\n","            sample_len = len(data)\n","        xs.append(torch.tensor(data).unsqueeze(0).float().cuda())\n","\n","    xs = torch.stack(xs).contiguous()\n","    print(f'xs size: {xs.size()}')\n","\n","    def save(x, decoder_ix, filepath):\n","        wav = inv_mu_law(x.cpu().numpy())\n","        print(f'X size: {x.shape}')\n","        print(f'X min: {x.min()}, max: {x.max()}')\n","\n","        if output_next_to_orig:\n","            save_audio(wav.squeeze(), filepath.parent / f'{filepath.stem}_{decoder_ix}.wav', rate=rate)\n","        else:\n","            save_audio(wav.squeeze(), output / str(decoder_ix) / filepath.with_suffix('.wav').name, rate=rate)\n","\n","    yy = {}\n","    with torch.no_grad():\n","        cc = []\n","        for xs_batch in torch.split(xs, batch_size):\n","            z, c = encoder(xs_batch)\n","            cc += [c]\n","        cc = torch.cat(cc, dim=0)\n","\n","        with timeit(\"Generation timer\"):\n","            for i, decoder_id in enumerate(decoder_ids):\n","                yy[decoder_id] = []\n","                decoder = decoders[i]\n","                for cc_batch in torch.split(cc, batch_size):\n","                    print(cc_batch.shape)\n","                    splits = torch.split(cc_batch, split_size, -1)\n","                    audio_data = []\n","                    decoder.reset()\n","                    for cond in tqdm.tqdm(splits):\n","                        audio_data += [decoder.generate(cond).cpu()]\n","                    audio_data = torch.cat(audio_data, -1)\n","                    yy[decoder_id] += [audio_data]\n","                yy[decoder_id] = torch.cat(yy[decoder_id], dim=0)\n","                del decoder\n","\n","    for decoder_ix, decoder_result in yy.items():\n","        for sample_result, filepath in zip(decoder_result, file_paths):\n","            save(sample_result, decoder_ix, filepath)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyke_FkSwN8I","executionInfo":{"status":"ok","timestamp":1620188888904,"user_tz":240,"elapsed":980419,"user":{"displayName":"Maggie Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkLiPqFHYr8xvDLuA4iaOhdQALYt_dLp0AcxzHmA=s64","userId":"10249436975890332274"}},"outputId":"b75c1efe-1df9-4ff3-f7d4-b31dce991d21"},"source":["checkpoint_args = Path('/content/checkpoints/musicnet_umtcpc/args.pth')\n","output = Path('/content/results')\n","checkpoint_lastmodel = Path('/content/checkpoints/musicnet_umtcpc/lastmodel')\n","\n","# Extract data samples to use as input for translation\n","data_samples(None, checkpoint_args, output, 1, 5000)\n","\n","files = [Path('/content/results')]\n","run_on_files_umtcpc(files, None, checkpoint_lastmodel, decoders=[0, 1, 2], batch_size=2, output_next_to_orig=True)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["2021-05-05 04:11:48,586 - INFO - Dataset created. 2 files, augmentation: False. Path: musicnet/preprocessed/Bach_Solo_Cello/test\n","2021-05-05 04:11:48,587 - INFO - Dataset created. 5 files, augmentation: False. Path: musicnet/preprocessed/Bach_Solo_Piano/test\n","\n","\n","\n","\n","\n","\n","100%|██████████| 1/1 [00:00<00:00, 92.51it/s]\n","\n","\n","\n","\n","\n","\n","100%|██████████| 1/1 [00:00<00:00, 73.81it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Starting\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["xs size: torch.Size([2, 1, 5000])\n","torch.Size([2, 64, 31])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","\n","\n","\n","\n","\n","\n","\n","Generating:   5%|▌         | 1/20 [00:17<05:31, 17.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  10%|█         | 2/20 [00:34<05:10, 17.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  15%|█▌        | 3/20 [00:50<04:46, 16.83s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  20%|██        | 4/20 [01:05<04:24, 16.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  25%|██▌       | 5/20 [01:21<04:04, 16.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  30%|███       | 6/20 [01:37<03:46, 16.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  35%|███▌      | 7/20 [01:53<03:29, 16.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  40%|████      | 8/20 [02:09<03:12, 16.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  45%|████▌     | 9/20 [02:25<02:56, 16.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  50%|█████     | 10/20 [02:41<02:40, 16.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  55%|█████▌    | 11/20 [02:57<02:25, 16.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  60%|██████    | 12/20 [03:13<02:08, 16.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  65%|██████▌   | 13/20 [03:29<01:52, 16.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  70%|███████   | 14/20 [03:45<01:36, 16.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  75%|███████▌  | 15/20 [04:01<01:19, 15.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  80%|████████  | 16/20 [04:17<01:03, 15.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  85%|████████▌ | 17/20 [04:32<00:47, 15.84s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  90%|█████████ | 18/20 [04:48<00:31, 15.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  95%|█████████▌| 19/20 [05:04<00:15, 15.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating: 100%|██████████| 20/20 [05:19<00:00, 16.00s/it]\n","INFO - 05/05/21 04:17:10 - 0:52:50 - 2 samples of 1.0 seconds length generated in 319.93580389022827 seconds.\n","\n","\n","\n","\n","\n","\n"," 50%|█████     | 1/2 [05:19<05:19, 319.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:   9%|▉         | 1/11 [00:15<02:35, 15.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  18%|█▊        | 2/11 [00:31<02:20, 15.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  27%|██▋       | 3/11 [00:46<02:04, 15.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  36%|███▋      | 4/11 [01:02<01:49, 15.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  45%|████▌     | 5/11 [01:18<01:34, 15.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  55%|█████▍    | 6/11 [01:33<01:18, 15.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  64%|██████▎   | 7/11 [01:49<01:02, 15.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  73%|███████▎  | 8/11 [02:05<00:46, 15.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  82%|████████▏ | 9/11 [02:20<00:31, 15.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  91%|█████████ | 10/11 [02:36<00:15, 15.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating: 100%|██████████| 11/11 [02:52<00:00, 15.66s/it]\n","INFO - 05/05/21 04:20:02 - 0:55:42 - 2 samples of 0.55 seconds length generated in 172.32513523101807 seconds.\n","\n","\n","\n","\n","\n","\n","100%|██████████| 2/2 [08:12<00:00, 246.22s/it]\n","\n","\n","\n","\n","\n","\n","  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["torch.Size([2, 64, 31])\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","Generating:   5%|▌         | 1/20 [00:15<04:58, 15.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  10%|█         | 2/20 [00:31<04:41, 15.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  15%|█▌        | 3/20 [00:46<04:26, 15.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  20%|██        | 4/20 [01:02<04:10, 15.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  25%|██▌       | 5/20 [01:18<03:54, 15.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  30%|███       | 6/20 [01:33<03:37, 15.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  35%|███▌      | 7/20 [01:49<03:21, 15.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  40%|████      | 8/20 [02:04<03:07, 15.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  45%|████▌     | 9/20 [02:20<02:51, 15.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  50%|█████     | 10/20 [02:36<02:37, 15.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  55%|█████▌    | 11/20 [02:52<02:21, 15.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  60%|██████    | 12/20 [03:07<02:05, 15.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  65%|██████▌   | 13/20 [03:23<01:50, 15.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  70%|███████   | 14/20 [03:39<01:34, 15.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  75%|███████▌  | 15/20 [03:54<01:18, 15.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  80%|████████  | 16/20 [04:10<01:02, 15.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  85%|████████▌ | 17/20 [04:26<00:47, 15.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  90%|█████████ | 18/20 [04:41<00:31, 15.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  95%|█████████▌| 19/20 [04:57<00:15, 15.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating: 100%|██████████| 20/20 [05:13<00:00, 15.67s/it]\n","INFO - 05/05/21 04:25:15 - 1:00:56 - 2 samples of 1.0 seconds length generated in 313.3970251083374 seconds.\n","\n","\n","\n","\n","\n","\n"," 50%|█████     | 1/2 [05:13<05:13, 313.41s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:   9%|▉         | 1/11 [00:15<02:35, 15.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  18%|█▊        | 2/11 [00:31<02:20, 15.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  27%|██▋       | 3/11 [00:46<02:04, 15.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  36%|███▋      | 4/11 [01:02<01:49, 15.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  45%|████▌     | 5/11 [01:18<01:34, 15.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  55%|█████▍    | 6/11 [01:34<01:18, 15.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  64%|██████▎   | 7/11 [01:49<01:02, 15.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  73%|███████▎  | 8/11 [02:05<00:46, 15.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  82%|████████▏ | 9/11 [02:20<00:31, 15.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating:  91%|█████████ | 10/11 [02:36<00:15, 15.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","Generating: 100%|██████████| 11/11 [02:52<00:00, 15.66s/it]\n","INFO - 05/05/21 04:28:08 - 1:03:48 - 2 samples of 0.55 seconds length generated in 172.2909059524536 seconds.\n","\n","\n","\n","\n","\n","\n","100%|██████████| 2/2 [08:05<00:00, 242.93s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Generation timer took 978345.251083374 ms\n","X size: torch.Size([1, 24800])\n","X min: 8, max: 244\n","X size: torch.Size([1, 24800])\n","X min: 1, max: 254\n","X size: torch.Size([1, 24800])\n","X min: 26, max: 227\n","X size: torch.Size([1, 24800])\n","X min: 23, max: 226\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FaJa9hwFuYky"},"source":[""],"execution_count":null,"outputs":[]}]}