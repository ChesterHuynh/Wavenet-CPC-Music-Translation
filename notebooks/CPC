{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CPC","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PpVqKFbFteQ9"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","from scipy.io import wavfile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eX5tIfCetqtv"},"source":["CPC + NCE loss"]},{"cell_type":"code","metadata":{"id":"oZDuGyyUtqZM"},"source":["class CPC(nn.Module):\n","    \"\"\"\n","    Creates a contrastive predictive coding model with a strided convolutional \n","    encoder and GRU RNN autoregressor as described by [1] and implemented in [2].\n","\n","    References\n","    ----------\n","    [1] van der Oord et al., \"Representation Learning with Contrastive \n","        Predictive Coding\", arXiv, 2019.\n","        https://arxiv.org/abs/1807.03748\n","    [2] Lai, \"Contrastive-Predictive-Coding-PyTorch\", GitHub.\n","        https://github.com/jefflai108/Contrastive-Predictive-Coding-PyTorch\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.encoder = nn.Sequential( # downsampling factor = 160\n","            nn.Conv1d(1, 512, kernel_size=10, stride=5, padding=3, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=8, stride=4, padding=2, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(512, 512, kernel_size=4, stride=2, padding=1, bias=False),\n","            nn.BatchNorm1d(512),\n","            nn.ReLU(inplace=True)\n","        )\n","        self.ar = nn.GRU(512, 256, num_layers=1, bidirectional=False, batch_first=True)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Parameters\n","        ----------\n","            x : B x 1 x L torch.Tensor\n","                Input batch of audio sequence with B samples and length L.\n","\n","        Returns\n","        -------\n","            z : B x (L // 160) x 512 torch.Tensor\n","                Encoded representation of audio sequence with 512 channels.\n","            c : B x (L // 160) x 256 torch.Tensor\n","                Context-encoded representation of audio sequence with 256 channels.\n","        \"\"\"\n","        # Use encoder to get sequence of latent representations z_t\n","        z = self.encoder(x)\n","        z = z.transpose(1,2)\n","\n","        # Use autoregressive model to compute context latent representation c_t\n","        c, _ = self.ar(z)\n","\n","        return z, c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgHK4bpWuC8h"},"source":["class InfoNCELoss(nn.Module):\n","    \"\"\"\n","    Creates a criterion that computes the InfoNCELoss as described in [1].\n","    \n","    Parameters\n","    ----------\n","        prediction_step : int\n","            Number of steps to predict into the future using context vector c\n","\n","    References\n","    ----------\n","    [1] van der Oord et al., \"Representation Learning with Contrastive \n","        Predictive Coding\", arXiv, 2019.\n","        https://arxiv.org/abs/1807.03748\n","    \"\"\"\n","    def __init__(self, prediction_step):\n","        super().__init__()\n","        self.prediction_step = prediction_step\n","        self.Wk = nn.ModuleList(\n","            nn.Linear(64, 512) for _ in range(prediction_step)\n","        )\n","\n","    def get_neg_z(self, z, k, t, n_replicates):\n","        \"\"\"\n","        Parameters\n","        ----------\n","            z : B x L x 512 torch.Tensor\n","                Encoded representation of audio sequence.\n","            k : int\n","                Number of time steps in the future for prediction\n","            t : B torch.Tensor\n","                Current time step for each sample in the batch\n","            n_replicates : int\n","                Number of repetitions of the negative sampling procedure\n","\n","        Returns\n","        -------\n","            neg_samples : B x L-1 x N_rep x 512 torch.Tensor\n","                Batch-wise average InfoNCE loss\n","        \"\"\"\n","        cur_device = z.get_device() if z.get_device() != -1 else \"cpu\"\n","\n","        neg_idx = torch.vstack([torch.cat([\n","            torch.arange(0, t_i + k),             # indices before t+k\n","            torch.arange(t_i + k + 1, z.size(1))  # indices after t+k\n","        ]) for t_i in t])\n","\n","        neg_samples = torch.vstack([z[i, neg_idx[i]].unsqueeze(0) for i in range(len(t))])\n","        neg_samples = torch.stack(\n","            [\n","                torch.index_select(neg_samples, 1, torch.randint(neg_samples.size(1), \n","                                                                 (neg_samples.size(1), )).to(cur_device))\n","                for i in range(n_replicates)\n","            ],\n","            2,\n","        )\n","        return neg_samples\n","        \n","    \n","    def forward(self, z, c, n_replicates):\n","        \"\"\"\n","        Parameters\n","        ----------\n","            z : B x L x 512 torch.Tensor\n","                Encoded representation of audio sequence.\n","            c : B x L x 256 torch.Tensor\n","                Context-encoded representation of audio sequence.\n","            n_replicates : int\n","                Number of times to make a set of negative samples.\n","        \n","        Returns\n","        -------\n","            loss : float Tensor\n","                Batch-wise average InfoNCE loss\n","        \"\"\"\n","        loss = 0\n","\n","        n_batches = z.size(0)\n","\n","        # Sample random t for each batch\n","        cur_device = z.get_device() if z.get_device() != -1 else \"cpu\"\n","        t = torch.randint(z.size(1) - self.prediction_step - 1, (n_batches,)).to(cur_device)\n","\n","        # Get context vector c_t\n","        c_t = c[torch.arange(n_batches), t] # B x 256\n","\n","        for k in range(1, self.prediction_step + 1):\n","            # Perform negative sampling\n","            neg_samples = self.get_neg_z(z, k, t, n_replicates)  # B x L-1 x N_rep x C\n","\n","            # Compute W_k * c_t\n","            linear = self.Wk[k - 1]  # 256 x C\n","            pred = linear(c_t) # B x C\n","\n","            # Get positive z_t+k sample\n","            pos_sample = z[torch.arange(n_batches), t+k]\n","\n","            # Positive sample: compute f_k(x_t+k, c_t)\n","            # Only take diagonal elements to get product between matched batches\n","            fk_pos = torch.diag(torch.matmul(pos_sample, pred.T)) # B (1-D tensor)\n","            fk_pos_rep = fk_pos.repeat(n_replicates).view(1, 1, n_replicates, fk_pos.size(0)) # 1 x 1 x N_rep x B\n","\n","            # Negative samples: compute f_k(x_j, c_t)\n","            # Only take diagonal elements to get products between matched batches\n","            fk_neg = torch.matmul(neg_samples, pred.T) # B x L-1 x N_rep x B\n","            fk_neg = torch.diagonal(fk_neg, dim1=0, dim2=-1).unsqueeze(0) # 1 x L-1 x N_rep x B\n","\n","            # Concatenate fk for positive and negative samples\n","            fk = torch.hstack([fk_pos_rep, fk_neg]) # 1 x L x N_rep x B\n","\n","            # Compute log softmax over all fk \n","            log_sm_fk = torch.nn.LogSoftmax(dim=1)(fk)  # 1 x L x N_rep x B\n","\n","            # Compute expected value of log softmaxes over replicates\n","            exp_log_sm_fk = torch.mean(log_sm_fk, dim=2)  # 1 x L x B    \n","\n","            # Update loss with log softmax element corresponding to positive sample\n","            loss -= exp_log_sm_fk[:, 0] # 1 x B\n","            \n","        # Divide by number of predicted steps\n","        loss /= self.prediction_step\n","\n","        # Average over batches\n","        loss = loss.sum() / n_batches\n","\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pC6667YC2m0l","executionInfo":{"status":"ok","timestamp":1620066751055,"user_tz":240,"elapsed":1569,"user":{"displayName":"Chester Huynh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2BRT271skWdp-WWHqZJnCWuewZ2iADwpruJyt_Q=s64","userId":"07499809855745885442"}},"outputId":"07bbb9f2-3c4f-464d-f840-14734102c987"},"source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","start = 44100 * 3\n","seq_len = 10000\n","\n","# samplerate, x = wavfile.read('/content/musicnet/1764.wav')\n","samplerate, x = wavfile.read('/content/musicnet/1727.wav')\n","x = x[start:start+seq_len]\n","x = torch.from_numpy(x).view(1, 1, -1)\n","x = x.to(device)\n","\n","# samplerate, x2 = wavfile.read('/content/musicnet/1766.wav')\n","samplerate, x2 = wavfile.read('/content/musicnet/1728.wav')\n","x2 = x2[start:start+seq_len]\n","x2 = torch.from_numpy(x2).view(1, 1, -1)\n","x2 = x2.to(device)\n","\n","batch = torch.cat([x, x2])\n","\n","batch.shape\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n","  if __name__ == '__main__':\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 1, 10000])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"U6624MgCzTw2"},"source":["torch.manual_seed(0)\n","model = CPC()\n","model = model.to(device)\n","z, c = model(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"498ajAwM3y8V","executionInfo":{"status":"ok","timestamp":1620066751306,"user_tz":240,"elapsed":1809,"user":{"displayName":"Chester Huynh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2BRT271skWdp-WWHqZJnCWuewZ2iADwpruJyt_Q=s64","userId":"07499809855745885442"}},"outputId":"4ae3a977-b106-49b6-a7cb-75a704a0527d"},"source":["# z.shape = (batch, encoding_len, channels)\n","z.shape, c.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([2, 62, 512]), torch.Size([2, 62, 256]))"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tP0bpc2_rxUl","executionInfo":{"status":"ok","timestamp":1620066751306,"user_tz":240,"elapsed":1802,"user":{"displayName":"Chester Huynh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2BRT271skWdp-WWHqZJnCWuewZ2iADwpruJyt_Q=s64","userId":"07499809855745885442"}},"outputId":"d1bfd29c-5153-48e3-a1d3-4cf14adcc1ff"},"source":["torch.manual_seed(0)\n","loss = InfoNCELoss(prediction_step=5)\n","l = loss(z, c, n_replicates=5)\n","l"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(8.4248, grad_fn=<DivBackward0>)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"LN7EszXh-aUh"},"source":[""],"execution_count":null,"outputs":[]}]}